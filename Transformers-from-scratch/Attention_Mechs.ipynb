{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3 (ipykernel)",
   "language": "python"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "Sg64jx10XwI3",
    "outputId": "6466ac93-37d3-4f9d-e424-70cdd42e1ccd",
    "ExecuteTime": {
     "end_time": "2025-01-25T05:16:51.435704Z",
     "start_time": "2025-01-25T05:16:49.457773Z"
    }
   },
   "source": [
    "#SCALED DOT PRODUCT\n",
    "from IPython import get_ipython\n",
    "from IPython.display import display\n",
    "from tensorflow import matmul, cast, float32, math\n",
    "from tensorflow.math import sqrt\n",
    "from tensorflow.keras.layers import Layer\n",
    "from tensorflow.keras.activations import softmax\n",
    "import numpy as np\n",
    "\n",
    "class DotProductAttention(Layer):\n",
    "  def __init__(self, **kwargs):\n",
    "    super().__init__(**kwargs)\n",
    "\n",
    "  def call(self, queries, keys, values, *, d_k, mask=None):\n",
    "    scores = matmul(queries, keys, transpose_b=True) / sqrt(cast(d_k, float32))\n",
    "    if mask is not None:\n",
    "      scores += -1e9 * mask\n",
    "    weights = softmax(scores)\n",
    "    return matmul(weights, values)\n",
    "\n",
    "batch_size = 32\n",
    "input_seq_length = 10\n",
    "d_k = 64\n",
    "d_v = 64\n",
    "\n",
    "random = np.random.default_rng(seed=42)\n",
    "queries = random.random((batch_size, input_seq_length, d_k))\n",
    "keys = random.random((batch_size, input_seq_length, d_k))\n",
    "values = random.random((batch_size, input_seq_length, d_v))\n",
    "attention = DotProductAttention()\n",
    "print(attention(queries, keys, values, d_k=d_k))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[0.46824056 0.55641305 0.46830386 ... 0.4930685  0.4061298  0.46411476]\n",
      "  [0.47598848 0.5551045  0.47800195 ... 0.49304226 0.4006043  0.47026026]\n",
      "  [0.47095588 0.55246687 0.47349647 ... 0.49247038 0.41551554 0.46566948]\n",
      "  ...\n",
      "  [0.47359407 0.5530097  0.48761857 ... 0.49078512 0.4073912  0.47809467]\n",
      "  [0.4738524  0.5515101  0.47469318 ... 0.48799846 0.40717867 0.47817174]\n",
      "  [0.4573552  0.55452013 0.4731847  ... 0.48771793 0.41125485 0.4576    ]]\n",
      "\n",
      " [[0.5257553  0.46964663 0.6492506  ... 0.54562765 0.62523377 0.49289626]\n",
      "  [0.51869744 0.48040384 0.6457132  ... 0.53287935 0.6220018  0.5044591 ]\n",
      "  [0.5311054  0.48170856 0.64107096 ... 0.54553026 0.62820685 0.4921141 ]\n",
      "  ...\n",
      "  [0.5314952  0.48453844 0.6366704  ... 0.5249854  0.6216751  0.50992715]\n",
      "  [0.52042365 0.4842645  0.64478606 ... 0.5371342  0.6203686  0.5011124 ]\n",
      "  [0.51877236 0.4815875  0.64000344 ... 0.5273335  0.62762994 0.5056677 ]]\n",
      "\n",
      " [[0.57879597 0.4952488  0.58734167 ... 0.59600276 0.670287   0.57462704]\n",
      "  [0.57163984 0.46866807 0.57322687 ... 0.5869684  0.66590536 0.5821251 ]\n",
      "  [0.58166564 0.47137827 0.57664037 ... 0.59543765 0.6703509  0.58260524]\n",
      "  ...\n",
      "  [0.56678414 0.46349686 0.5741636  ... 0.585176   0.6688597  0.5810943 ]\n",
      "  [0.5625993  0.47281584 0.5796631  ... 0.5718025  0.6826521  0.57993925]\n",
      "  [0.56269234 0.46991956 0.57713026 ... 0.5713144  0.678899   0.5815345 ]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0.59185946 0.5037147  0.48920795 ... 0.49912474 0.5193292  0.44257247]\n",
      "  [0.59446806 0.5018795  0.48390767 ... 0.507101   0.5203186  0.4513293 ]\n",
      "  [0.5994351  0.5003228  0.4904711  ... 0.5067741  0.51234823 0.44971162]\n",
      "  ...\n",
      "  [0.59888643 0.51105756 0.47838598 ... 0.5241687  0.50811034 0.44435114]\n",
      "  [0.59462494 0.4931421  0.48902366 ... 0.5243883  0.5071588  0.44299516]\n",
      "  [0.6078781  0.4962665  0.48526418 ... 0.52623224 0.5060221  0.44728178]]\n",
      "\n",
      " [[0.53501385 0.5384201  0.6192722  ... 0.46005803 0.49126065 0.46791288]\n",
      "  [0.5399568  0.52438086 0.62347984 ... 0.45036897 0.489887   0.46677804]\n",
      "  [0.5382602  0.5239292  0.62087303 ... 0.4490124  0.48433793 0.4667065 ]\n",
      "  ...\n",
      "  [0.53283656 0.53147143 0.620358   ... 0.44892344 0.48647514 0.47543448]\n",
      "  [0.5463698  0.51945055 0.61222184 ... 0.44945577 0.47570434 0.47964767]\n",
      "  [0.54493695 0.5243833  0.6126049  ... 0.45642942 0.4902647  0.4641792 ]]\n",
      "\n",
      " [[0.48972577 0.61490774 0.4827907  ... 0.54042614 0.47191083 0.44551888]\n",
      "  [0.49905464 0.602793   0.49308947 ... 0.5288796  0.46995604 0.45995677]\n",
      "  [0.4789341  0.62047666 0.4711766  ... 0.5472053  0.4794891  0.44945213]\n",
      "  ...\n",
      "  [0.497671   0.6206938  0.4613046  ... 0.53163147 0.49636278 0.4426992 ]\n",
      "  [0.49511722 0.611762   0.48759618 ... 0.53429717 0.46429306 0.45405388]\n",
      "  [0.49077258 0.6169313  0.46535453 ... 0.530382   0.4803736  0.45963296]]], shape=(32, 10, 64), dtype=float32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-25 10:46:51.360898: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M2 Pro\n",
      "2025-01-25 10:46:51.360925: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 16.00 GB\n",
      "2025-01-25 10:46:51.360929: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 5.33 GB\n",
      "2025-01-25 10:46:51.360957: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2025-01-25 10:46:51.360973: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "source": [
    "from IPython import get_ipython\n",
    "from IPython.display import display\n",
    "# %%\n",
    "from tensorflow import math, matmul, reshape, shape, transpose, cast, float32, concat\n",
    "from tensorflow.keras.layers import Dense, Layer\n",
    "from tensorflow.keras.backend import softmax\n",
    "# Implementing the Scaled-Dot Product Attention\n",
    "\n",
    "class DotProductAttention(Layer):\n",
    "  def __init__(self, **kwargs):\n",
    "    super().__init__(**kwargs)\n",
    "\n",
    "  def call(self, queries, keys, values, mask=None):\n",
    "    d_k = queries.shape[-1]\n",
    "    # Scoring the queries against the keys after transposing the latter, and scaling\n",
    "    scores = matmul(queries, keys, transpose_b=True) / math.sqrt(cast(d_k, float32))\n",
    "    # Apply mask to the attention scores\n",
    "    if mask is not None:\n",
    "      scores += -1e9 * mask\n",
    "    # Computing the weights by a softmax operation\n",
    "    weights = softmax(scores)\n",
    "    # Computing the attention by a weighted sum of the value vectors\n",
    "    return matmul(weights, values)\n",
    "    # Implementing the Multi-Head Attention\n",
    "\n",
    "class MultiHeadAttention(Layer):\n",
    "  def __init__(self, h, d_k, d_v, d_model, **kwargs):\n",
    "    super().__init__(**kwargs)\n",
    "    self.attention = DotProductAttention() # Scaled dot product attention\n",
    "    self.heads = h # Number of attention heads to use\n",
    "    self.d_k = d_k # Dimensionality of the linearly projected queries and keys\n",
    "    self.d_v = d_v # Dimensionality of the linearly projected values\n",
    "    self.d_model = d_model # Dimensionality of the model\n",
    "    self.W_q = Dense(d_k) # Learned projection matrix for the queries\n",
    "    self.W_k = Dense(d_k) # Learned projection matrix for the keys\n",
    "    self.W_v = Dense(d_v) # Learned projection matrix for the values\n",
    "    self.W_o = Dense(d_model) # Learned projection matrix for the multi-head output\n",
    "\n",
    "  def reshape_tensor(self, x, heads, flag):\n",
    "    if flag:\n",
    "      # Tensor shape after reshaping and transposing:\n",
    "      # (batch_size, heads, seq_length, -1)\n",
    "      x = reshape(x, shape=(shape(x)[0], shape(x)[1], heads, -1))\n",
    "      x = transpose(x, perm=(0, 2, 1, 3))\n",
    "    else:\n",
    "        x = transpose(x, perm=(0, 2, 1, 3))\n",
    "        x_shape = shape(x)\n",
    "        new_shape = (x_shape[0], x_shape[1], x_shape[2] * x_shape[3])\n",
    "        x = reshape(x, new_shape)\n",
    "\n",
    "    return x\n",
    "\n",
    "  def call(self, queries, keys, values, mask=None):\n",
    "    # Rearrange the queries to be able to compute all heads in parallel\n",
    "    q_reshaped = self.reshape_tensor(self.W_q(queries), self.heads, True)\n",
    "    # Resulting tensor shape: (batch_size, heads, input_seq_length, -1)\n",
    "    # Rearrange the keys to be able to compute all heads in parallel\n",
    "    k_reshaped = self.reshape_tensor(self.W_k(keys), self.heads, True)\n",
    "    # Resulting tensor shape: (batch_size, heads, input_seq_length, -1)\n",
    "    # Rearrange the values to be able to compute all heads in parallel\n",
    "    v_reshaped = self.reshape_tensor(self.W_v(values), self.heads, True)\n",
    "    # Resulting tensor shape: (batch_size, heads, input_seq_length, -1)\n",
    "    # Compute the multi-head attention output using the reshaped queries,\n",
    "    # keys, and values\n",
    "    o_reshaped = self.attention(q_reshaped, k_reshaped, v_reshaped, mask=mask)\n",
    "    # Resulting tensor shape: (batch_size, heads, input_seq_length, -1)\n",
    "    # Rearrange back the output into concatenated form\n",
    "    output = self.reshape_tensor(o_reshaped, self.heads, False)\n",
    "    # Resulting tensor shape: (batch_size, input_seq_length, d_model)\n",
    "    return self.W_o(output)\n",
    "# %%\n",
    "from numpy import random\n",
    "input_seq_length = 5 # Maximum length of the input sequence\n",
    "h = 8 # Number of self-attention heads\n",
    "d_k = 64 # Dimensionality of the linearly projected queries and keys\n",
    "d_v = 64 # Dimensionality of the linearly projected values\n",
    "d_model = 512 # Dimensionality of the model sub-layers' outputs\n",
    "batch_size = 64 # Batch size from the training process\n",
    "queries = random.random((batch_size, input_seq_length, d_k))\n",
    "keys = random.random((batch_size, input_seq_length, d_k))\n",
    "values = random.random((batch_size, input_seq_length, d_v))\n",
    "multihead_attention = MultiHeadAttention(h, d_k, d_v, d_model)\n",
    "print(multihead_attention(queries, keys, values))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "mGph6WV6X-mP",
    "outputId": "5d35314e-fd2f-492e-f30f-7d201c67d151",
    "ExecuteTime": {
     "end_time": "2025-01-25T05:16:51.522488Z",
     "start_time": "2025-01-25T05:16:51.437147Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[ 0.13987488  0.30761647  0.02906834 ... -0.16418055  0.00747937\n",
      "    0.134643  ]\n",
      "  [ 0.1417179   0.3093925   0.02572403 ... -0.17111947  0.0153925\n",
      "    0.14233968]\n",
      "  [ 0.12901346  0.3089638   0.02434182 ... -0.16623019  0.00347683\n",
      "    0.13525382]\n",
      "  [ 0.14464812  0.31266147  0.0334737  ... -0.16386218  0.01659877\n",
      "    0.14627695]\n",
      "  [ 0.13608232  0.30922857  0.03331729 ... -0.17057301  0.01323724\n",
      "    0.14136611]]\n",
      "\n",
      " [[ 0.03687583  0.5083465  -0.06797247 ... -0.37163785 -0.05848521\n",
      "    0.18905208]\n",
      "  [ 0.0328756   0.5113885  -0.06789006 ... -0.37105635 -0.06338108\n",
      "    0.18322463]\n",
      "  [ 0.03317652  0.5156679  -0.06193725 ... -0.36009657 -0.0592146\n",
      "    0.191917  ]\n",
      "  [ 0.0322786   0.5081383  -0.07410406 ... -0.36320326 -0.05663864\n",
      "    0.1925717 ]\n",
      "  [ 0.02948342  0.51423    -0.06779737 ... -0.3678403  -0.06324235\n",
      "    0.18439998]]\n",
      "\n",
      " [[ 0.20744371  0.307809    0.00739995 ... -0.33406168  0.01336134\n",
      "    0.10662279]\n",
      "  [ 0.19506627  0.31260258 -0.00123618 ... -0.33562514  0.01799057\n",
      "    0.11310706]\n",
      "  [ 0.20097195  0.31267923 -0.00055916 ... -0.33290654  0.01490945\n",
      "    0.11040695]\n",
      "  [ 0.1927303   0.3191132   0.00579533 ... -0.33912903  0.01240392\n",
      "    0.11212751]\n",
      "  [ 0.18771511  0.32846272  0.00208702 ... -0.3294151   0.01884611\n",
      "    0.11913548]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[ 0.12120677  0.4773422   0.07369703 ... -0.3521444  -0.05739355\n",
      "    0.02496319]\n",
      "  [ 0.12471952  0.4772679   0.07333189 ... -0.3494587  -0.04803581\n",
      "    0.03820481]\n",
      "  [ 0.12352002  0.47491896  0.0707722  ... -0.34680077 -0.05198615\n",
      "    0.04066078]\n",
      "  [ 0.12621026  0.4782588   0.06862137 ... -0.3525374  -0.0515391\n",
      "    0.0338375 ]\n",
      "  [ 0.12469858  0.47303915  0.06681833 ... -0.34718105 -0.05371258\n",
      "    0.04216955]]\n",
      "\n",
      " [[-0.0300671   0.33891082 -0.02578541 ... -0.27359292 -0.03832617\n",
      "    0.10275678]\n",
      "  [-0.02623791  0.34596184 -0.03085649 ... -0.27053455 -0.04195233\n",
      "    0.10149809]\n",
      "  [-0.03341916  0.34867442 -0.02885546 ... -0.27137658 -0.03928454\n",
      "    0.10504379]\n",
      "  [-0.02716868  0.3380121  -0.03692352 ... -0.2693877  -0.04466875\n",
      "    0.09348738]\n",
      "  [-0.02918844  0.3379459  -0.03339949 ... -0.2702534  -0.04388905\n",
      "    0.09482542]]\n",
      "\n",
      " [[ 0.06231261  0.31893507  0.02996019 ... -0.28630105  0.00948099\n",
      "    0.09977167]\n",
      "  [ 0.0711072   0.3298446   0.04871514 ... -0.2788809   0.0060866\n",
      "    0.10542115]\n",
      "  [ 0.07005918  0.3257642   0.03549008 ... -0.2866312   0.00544521\n",
      "    0.10772251]\n",
      "  [ 0.06477864  0.32003358  0.04122989 ... -0.2797689   0.00534227\n",
      "    0.10840262]\n",
      "  [ 0.06912725  0.31717375  0.03414087 ... -0.28073987  0.00807995\n",
      "    0.11362323]]], shape=(64, 5, 512), dtype=float32)\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "class BahdanauAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.W1 = tf.keras.layers.Dense(units)  # Dense layer for the encoder hidden states\n",
    "        self.W2 = tf.keras.layers.Dense(units)  # Dense layer for the decoder hidden state\n",
    "        self.V = tf.keras.layers.Dense(1)       # Dense layer to compute alignment scores\n",
    "\n",
    "    def call(self, query, values):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            query: Decoder hidden state (shape: [batch_size, hidden_size]).\n",
    "            values: Encoder outputs (shape: [batch_size, seq_len, hidden_size]).\n",
    "        Returns:\n",
    "            context_vector: Weighted sum of encoder outputs (shape: [batch_size, hidden_size]).\n",
    "            attention_weights: Attention weights (shape: [batch_size, seq_len]).\n",
    "        \"\"\"\n",
    "        # Add time axis to query for broadcasting (shape: [batch_size, 1, hidden_size])\n",
    "        query_with_time_axis = tf.expand_dims(query, 1)\n",
    "\n",
    "        # Compute the alignment scores (shape: [batch_size, seq_len, 1])\n",
    "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(query_with_time_axis)))\n",
    "\n",
    "        # Remove the last axis (shape: [batch_size, seq_len])\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "\n",
    "        # Compute the context vector as the weighted sum of values (shape: [batch_size, hidden_size])\n",
    "        context_vector = tf.reduce_sum(attention_weights * values, axis=1)\n",
    "\n",
    "        return context_vector, attention_weights\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Define batch size, sequence length, and hidden size\n",
    "    batch_size = 64\n",
    "    seq_len = 10\n",
    "    hidden_size = 256\n",
    "    attention_units = 128\n",
    "\n",
    "    # Instantiate the attention layer\n",
    "    attention = BahdanauAttention(units=attention_units)\n",
    "\n",
    "    # Simulated encoder outputs (values) and decoder hidden state (query)\n",
    "    encoder_outputs = tf.random.normal([batch_size, seq_len, hidden_size])\n",
    "    decoder_hidden_state = tf.random.normal([batch_size, hidden_size])\n",
    "\n",
    "    # Apply the attention mechanism\n",
    "    context_vector, attention_weights = attention(decoder_hidden_state, encoder_outputs)\n",
    "\n",
    "    print(\"Context vector shape:\", context_vector.shape)  # Expected: [batch_size, hidden_size]\n",
    "    print(\"Attention weights shape:\", attention_weights.shape)  # Expected: [batch_size, seq_len]\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "4PxLmp2TYCnj",
    "outputId": "e3965fae-3e95-4140-d02e-674e1f8f37a9",
    "ExecuteTime": {
     "end_time": "2025-01-25T05:16:51.608396Z",
     "start_time": "2025-01-25T05:16:51.523509Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context vector shape: (64, 256)\n",
      "Attention weights shape: (64, 10, 1)\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Layer, Dense\n",
    "\n",
    "class LuongAttention(Layer):\n",
    "    def __init__(self, attention_type, hidden_size):\n",
    "        super(LuongAttention, self).__init__()\n",
    "        self.attention_type = attention_type\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        if attention_type == \"general\":\n",
    "            self.attention_weight = Dense(hidden_size)\n",
    "        elif attention_type == \"concat\":\n",
    "            self.attention_weight = Dense(hidden_size)\n",
    "            self.v = tf.Variable(tf.random.normal([hidden_size]), trainable=True)\n",
    "\n",
    "    def score(self, hidden, encoder_outputs):\n",
    "        if self.attention_type == \"dot\":\n",
    "            # Dot product between hidden state and encoder outputs\n",
    "            return tf.matmul(encoder_outputs, tf.expand_dims(hidden, axis=-1))[:, :, 0]\n",
    "\n",
    "        elif self.attention_type == \"general\":\n",
    "            # Linear transformation followed by dot product\n",
    "            energy = self.attention_weight(encoder_outputs)\n",
    "            return tf.matmul(energy, tf.expand_dims(hidden, axis=-1))[:, :, 0]\n",
    "\n",
    "        elif self.attention_type == \"concat\":\n",
    "            # Concatenate hidden state with encoder outputs\n",
    "            hidden_expanded = tf.expand_dims(hidden, axis=1)\n",
    "            hidden_expanded = tf.tile(hidden_expanded, [1, tf.shape(encoder_outputs)[1], 1])\n",
    "            concat_input = tf.concat([hidden_expanded, encoder_outputs], axis=-1)\n",
    "            energy = tf.tanh(self.attention_weight(concat_input))\n",
    "            return tf.reduce_sum(energy * self.v, axis=2)\n",
    "\n",
    "        else:\n",
    "            raise ValueError(\"Unknown attention type: {}\".format(self.attention_type))\n",
    "\n",
    "    def call(self, hidden, encoder_outputs):\n",
    "        # Compute alignment scores\n",
    "        alignment_scores = self.score(hidden, encoder_outputs)\n",
    "\n",
    "        # Softmax normalization to obtain attention weights\n",
    "        attention_weights = tf.nn.softmax(alignment_scores, axis=1)\n",
    "\n",
    "        # Compute the context vector as the weighted sum of encoder outputs\n",
    "        context_vector = tf.matmul(tf.expand_dims(attention_weights, axis=1), encoder_outputs)\n",
    "        context_vector = tf.squeeze(context_vector, axis=1)\n",
    "\n",
    "        return context_vector, attention_weights\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    batch_size = 2\n",
    "    seq_len = 5\n",
    "    hidden_size = 10\n",
    "\n",
    "    # Simulated inputs\n",
    "    hidden = tf.random.normal([batch_size, hidden_size])  # Decoder hidden state\n",
    "    encoder_outputs = tf.random.normal([batch_size, seq_len, hidden_size])  # Encoder outputs\n",
    "\n",
    "    # Instantiate Luong Attention (dot, general, or concat)\n",
    "    attention_type = \"dot\"  # Options: \"dot\", \"general\", \"concat\"\n",
    "    attention_layer = LuongAttention(attention_type, hidden_size)\n",
    "\n",
    "    # Forward pass\n",
    "    context_vector, attention_weights = attention_layer(hidden, encoder_outputs)\n",
    "\n",
    "    print(\"Context vector:\", context_vector.numpy())\n",
    "    print(\"Attention weights:\", attention_weights.numpy())\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "4JGYDm2yYEX9",
    "outputId": "2c631a66-477d-45c5-e293-0376d4456fbe",
    "ExecuteTime": {
     "end_time": "2025-01-25T05:16:51.639698Z",
     "start_time": "2025-01-25T05:16:51.609006Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context vector: [[-1.2955989   0.6636053   0.03890432  0.89639723  0.4319114  -0.52019006\n",
      "  -0.09344348  1.508288   -0.1414347  -0.24801174]\n",
      " [ 1.0715642   0.39579365  0.37740907 -0.7698864   0.5327158   0.24594975\n",
      "   0.7296208   0.40479633 -0.6249022  -1.0270318 ]]\n",
      "Attention weights: [[9.4991928e-01 8.8936475e-04 5.8627836e-03 5.9943022e-03 3.7334323e-02]\n",
      " [9.1376507e-01 9.7129603e-05 8.4337562e-02 9.2828377e-05 1.7074916e-03]]\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-25T05:16:51.642560Z",
     "start_time": "2025-01-25T05:16:51.641164Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": 4
  }
 ]
}
