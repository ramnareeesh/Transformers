{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52a67d7ea3e2440d",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-07T09:20:12.518893Z",
     "start_time": "2025-01-07T09:20:12.516241Z"
    },
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import random\n",
    "import numpy as np\n",
    "from keras.layers import LayerNormalization, Layer, Dense, ReLU, Dropout,TextVectorization, Embedding\n",
    "from keras.backend import softmax \n",
    "from tensorflow import math, matmul, reshape, shape, transpose, cast, float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4fd4d0f50ce881de",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-07T09:20:27.082930Z",
     "start_time": "2025-01-07T09:20:27.064623Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.15.0'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras\n",
    "keras.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f856f7d66f9850",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### The Word embedding and Positional encoding layer(uses sinusoidal encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6112b64b5b099080",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-07T05:31:16.026433Z",
     "start_time": "2025-01-07T05:31:16.023440Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "class PositionEmbeddingFixedWeights(Layer):\n",
    "    def __init__(self, seq_length, vocab_size, output_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        word_embedding_matrix = self.get_position_encoding(vocab_size, output_dim)\n",
    "        pos_embedding_matrix = self.get_position_encoding(seq_length, output_dim)\n",
    "        self.word_embedding_layer = Embedding(input_dim=vocab_size, output_dim=output_dim,weights=[word_embedding_matrix],trainable=False)\n",
    "        self.position_embedding_layer = Embedding(input_dim=seq_length, output_dim=output_dim,weights=[pos_embedding_matrix],trainable=False)\n",
    "    \n",
    "    def get_position_encoding(self, seq_len, d, n=10000):\n",
    "        P = np.zeros((seq_len, d))\n",
    "        for k in range(seq_len):\n",
    "            for i in np.arange(int(d/2)):\n",
    "                denominator = np.power(n, 2*i/d)\n",
    "                P[k, 2*i] = np.sin(k/denominator)\n",
    "                P[k, 2*i+1] = np.cos(k/denominator)\n",
    "        return P\n",
    "    \n",
    "    \n",
    "    def call(self, inputs):\n",
    "        position_indices = tf.range(tf.shape(inputs)[-1])\n",
    "        embedded_words = self.word_embedding_layer(inputs)\n",
    "        embedded_indices = self.position_embedding_layer(position_indices)\n",
    "        return embedded_words + embedded_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de87d5b1d87ede80",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Attention mechanism"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "964d63b395695f9f",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "#### Single head attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bad1c350fb64f87b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-07T05:31:18.513475Z",
     "start_time": "2025-01-07T05:31:18.508256Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "class DotProductAttention(Layer):\n",
    "    def __init__(self,**kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "    \n",
    "    def call(self,queries,keys,values,d_k,mask = None):\n",
    "        \n",
    "        # Scoring the queries against the keys after transposing the latter, and scaling\n",
    "        scores = matmul(queries, keys, transpose_b=True) / math.sqrt(cast(d_k, float32))\n",
    "        \n",
    "        # Apply mask to the attention scores\n",
    "        if mask is not None:\n",
    "            scores += -1e9 * mask\n",
    "        \n",
    "        # Computing the weights by a softmax operation\n",
    "        weights = softmax(scores)\n",
    "        \n",
    "        # Computing the attention by a weighted sum of the value vectors\n",
    "        return matmul(weights, values)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "978afc96cc7b8c2b",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "#### Multi - head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "75b90c6dc030364",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-07T05:31:22.784464Z",
     "start_time": "2025-01-07T05:31:22.778746Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttention(Layer):\n",
    "    def __init__(self, h, d_k, d_v, d_model, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.attention = DotProductAttention() # Scaled dot product attention\n",
    "        self.heads = h # Number of attention heads to use\n",
    "        self.d_k = d_k # Dimensionality of the linearly projected queries and keys\n",
    "        self.d_v = d_v # Dimensionality of the linearly projected values\n",
    "        self.d_model = d_model # Dimensionality of the model\n",
    "        self.W_q = Dense(d_k) # Learned projection matrix for the queries\n",
    "        self.W_k = Dense(d_k) # Learned projection matrix for the keys\n",
    "        self.W_v = Dense(d_v) # Learned projection matrix for the values\n",
    "        self.W_o = Dense(d_model) # Learned projection matrix for the multi-head output\n",
    "    def reshape_tensor(self, x, heads, flag):\n",
    "        if flag:\n",
    "            # Tensor shape after reshaping and transposing:\n",
    "            # (batch_size, heads, seq_length, -1)\n",
    "            x = reshape(x, shape=(shape(x)[0], shape(x)[1], heads, -1))\n",
    "            x = transpose(x, perm=(0, 2, 1, 3))\n",
    "        else:\n",
    "            # Reverting the reshaping and transposing operations:\n",
    "            # (batch_size, seq_length, d_k)\n",
    "            x = transpose(x, perm=(0, 2, 1, 3))\n",
    "            x = reshape(x, shape=(shape(x)[0], shape(x)[1], self.d_k))\n",
    "        return x\n",
    "    \n",
    "    def call(self, queries, keys, values, mask=None):\n",
    "        # Rearrange the queries to be able to compute all heads in parallel\n",
    "        q_reshaped = self.reshape_tensor(self.W_q(queries), self.heads, True)\n",
    "        # Resulting tensor shape: (batch_size, heads, input_seq_length, -1)\n",
    "        # Rearrange the keys to be able to compute all heads in parallel\n",
    "        k_reshaped = self.reshape_tensor(self.W_k(keys), self.heads, True)\n",
    "        # Resulting tensor shape: (batch_size, heads, input_seq_length, -1)\n",
    "        # Rearrange the values to be able to compute all heads in parallel\n",
    "        v_reshaped = self.reshape_tensor(self.W_v(values), self.heads, True)\n",
    "        # Resulting tensor shape: (batch_size, heads, input_seq_length, -1)\n",
    "        # Compute the multi-head attention output using the reshaped queries,\n",
    "        # keys, and values\n",
    "        o_reshaped = self.attention(q_reshaped, k_reshaped, v_reshaped, self.d_k, mask)\n",
    "        # Resulting tensor shape: (batch_size, heads, input_seq_length, -1)\n",
    "        # Rearrange back the output into concatenated form\n",
    "        output = self.reshape_tensor(o_reshaped, self.heads, False)\n",
    "        # Resulting tensor shape: (batch_size, input_seq_length, d_v)\n",
    "        # Apply one final linear projection to the output to generate the multi-head\n",
    "        # attention. Resulting tensor shape: (batch_size, input_seq_length, d_model)\n",
    "        return self.W_o(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbbcd512899b0b31",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Normalization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b513bbf956d52b65",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Implementing the Add & Norm Layer\n",
    "class AddNormalization(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.layer_norm = LayerNormalization() # Layer normalization layer\n",
    "    def call(self, x, sublayer_x):\n",
    "        # The sublayer input and output need to be of the same shape to be summed\n",
    "        add = x + sublayer_x\n",
    "        # Apply layer normalization to the sum\n",
    "        return self.layer_norm(add)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6cfcff982e75aec",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Feed forward layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6534f1f393696934",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-07T05:32:09.065885Z",
     "start_time": "2025-01-07T05:32:09.062409Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Implementing the Feed-Forward Layer\n",
    "class FeedForward(Layer):\n",
    "    def __init__(self, d_ff, d_model, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.fully_connected1 = Dense(d_ff) # First fully connected layer\n",
    "        self.fully_connected2 = Dense(d_model) # Second fully connected layer\n",
    "        self.activation = ReLU() # ReLU activation layer\n",
    "        \n",
    "    def call(self, x):\n",
    "        # The input is passed into the two fully-connected layers, with a ReLU in between\n",
    "        x_fc1 = self.fully_connected1(x)\n",
    "        return self.fully_connected2(self.activation(x_fc1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9442ac7ddf3c2089",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Encoder stack"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1015fe4eed401c6",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "#### A single Encoder layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e882409af95711b2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-07T05:32:38.550232Z",
     "start_time": "2025-01-07T05:32:38.544109Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Implementing the Encoder Layer\n",
    "class EncoderLayer(Layer):\n",
    "    def __init__(self, h, d_k, d_v, d_model, d_ff, rate, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.multihead_attention = MultiHeadAttention(h, d_k, d_v, d_model)\n",
    "        self.dropout1 = Dropout(rate)\n",
    "        self.add_norm1 = AddNormalization()\n",
    "        self.feed_forward = FeedForward(d_ff, d_model)\n",
    "        self.dropout2 = Dropout(rate)\n",
    "        self.add_norm2 = AddNormalization()\n",
    "    \n",
    "    def call(self, x, padding_mask, training):\n",
    "        # Multi-head attention layer\n",
    "        multihead_output = self.multihead_attention(x, x, x, padding_mask)\n",
    "        \n",
    "        # Expected output shape = (batch_size, sequence_length, d_model)\n",
    "        # Add in a dropout layer\n",
    "        multihead_output = self.dropout1(multihead_output, training=training)\n",
    "        # Followed by an Add & Norm layer\n",
    "        addnorm_output = self.add_norm1(x, multihead_output)\n",
    "        # Expected output shape = (batch_size, sequence_length, d_model)\n",
    "        # Followed by a fully connected layer\n",
    "        feedforward_output = self.feed_forward(addnorm_output)\n",
    "        # Expected output shape = (batch_size, sequence_length, d_model)\n",
    "        # Add in another dropout layer\n",
    "        feedforward_output = self.dropout2(feedforward_output, training=training)\n",
    "        # Followed by another Add & Norm layer\n",
    "        return self.add_norm2(addnorm_output, feedforward_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3af7de31c7e3dc4",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "#### The stacked encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d5d2e63cc9385ea6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-27T10:06:15.683308Z",
     "start_time": "2024-10-27T10:06:15.681421Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Implementing the Encoder\n",
    "class Encoder(Layer):\n",
    "    def __init__(self, vocab_size, sequence_length, h, d_k, d_v, d_model, d_ff, n, rate,\n",
    "    **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.pos_encoding = PositionEmbeddingFixedWeights(sequence_length, vocab_size,\n",
    "        d_model)\n",
    "        self.dropout = Dropout(rate)\n",
    "        self.encoder_layer = [EncoderLayer(h, d_k, d_v, d_model, d_ff, rate) for _ in range(n)]\n",
    "    \n",
    "    def call(self, input_sentence, padding_mask, training):\n",
    "        # Generate the positional encoding\n",
    "        pos_encoding_output = self.pos_encoding(input_sentence)\n",
    "        # Expected output shape = (batch_size, sequence_length, d_model)\n",
    "        # Add in a dropout layer\n",
    "        x = self.dropout(pos_encoding_output, training=training)\n",
    "        # Pass on the positional encoded values to each encoder layer\n",
    "        for i, layer in enumerate(self.encoder_layer):\n",
    "            x = layer(x, padding_mask, training)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dae1c4979f9770c",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "347d801d45a3d3c3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-27T10:06:23.691740Z",
     "start_time": "2024-10-27T10:06:23.688008Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "h = 8 # Number of self-attention heads\n",
    "d_k = 64 # Dimensionality of the linearly projected queries and keys\n",
    "d_v = 64 # Dimensionality of the linearly projected values\n",
    "d_ff = 2048 # Dimensionality of the inner fully connected layer\n",
    "d_model = 512 # Dimensionality of the model sub-layers' outputs\n",
    "n = 6 # Number of layers in the encoder stack\n",
    "batch_size = 64 # Batch size from the training process\n",
    "dropout_rate = 0.1 # Frequency of dropping the input units in the dropout layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dffed308a2b98eb9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-27T10:07:34.474765Z",
     "start_time": "2024-10-27T10:07:34.473556Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "enc_vocab_size = 20 # Vocabulary size for the encoder\n",
    "input_seq_length = 5 # Maximum length of the input sequence\n",
    "input_seq = np.random.rand(batch_size, input_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "273885e14713240d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-27T10:07:37.154595Z",
     "start_time": "2024-10-27T10:07:37.144901Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.15919889e-01, 4.91635944e-01, 4.40446882e-01, 6.29453773e-01,\n",
       "        3.66986749e-01],\n",
       "       [2.93714294e-01, 6.36222268e-01, 6.15577773e-01, 3.46301441e-01,\n",
       "        9.73837110e-01],\n",
       "       [3.83080686e-01, 2.36675677e-01, 7.23320078e-04, 3.84908616e-01,\n",
       "        7.99711894e-01],\n",
       "       [5.17823415e-01, 9.31756226e-01, 1.33404444e-01, 7.85592246e-01,\n",
       "        4.95811953e-03],\n",
       "       [1.10830221e-01, 8.96743341e-01, 7.87448103e-01, 2.73020536e-01,\n",
       "        3.20464169e-01],\n",
       "       [1.85161186e-01, 7.39244939e-01, 8.69971472e-01, 5.27118878e-01,\n",
       "        9.23751793e-01],\n",
       "       [7.89240884e-01, 5.39702157e-01, 7.91981697e-01, 6.88402317e-01,\n",
       "        2.84742420e-01],\n",
       "       [3.24290932e-01, 9.42433417e-01, 3.79823536e-01, 6.49576823e-01,\n",
       "        9.53947588e-01],\n",
       "       [4.38068550e-01, 6.06056482e-01, 9.21083213e-01, 2.35299504e-01,\n",
       "        6.50360131e-01],\n",
       "       [6.86218976e-01, 2.32017896e-01, 4.04086840e-01, 9.82082295e-01,\n",
       "        6.57499766e-01],\n",
       "       [4.53202830e-02, 4.89210435e-02, 1.79528633e-01, 2.94011018e-01,\n",
       "        8.76020948e-01],\n",
       "       [8.52423126e-01, 8.01929178e-01, 6.32709491e-01, 8.42035636e-01,\n",
       "        7.96113170e-01],\n",
       "       [3.99475161e-01, 1.06650163e-01, 8.21417298e-01, 7.63483718e-01,\n",
       "        9.68771946e-01],\n",
       "       [5.65963211e-01, 5.87702951e-01, 1.51087089e-01, 9.78995625e-01,\n",
       "        1.17036546e-01],\n",
       "       [3.00984533e-01, 6.61592372e-02, 7.40821391e-01, 1.56569917e-01,\n",
       "        3.05788421e-01],\n",
       "       [7.45651989e-01, 5.97300045e-01, 4.62775576e-01, 4.56985698e-01,\n",
       "        9.84197346e-01],\n",
       "       [2.02788793e-01, 4.06961363e-01, 5.41755719e-01, 3.43538814e-01,\n",
       "        3.56436801e-01],\n",
       "       [7.61774549e-01, 1.49542822e-01, 1.81227283e-01, 5.76891421e-01,\n",
       "        5.88530552e-01],\n",
       "       [9.02182956e-01, 4.47264891e-01, 5.24729980e-01, 1.64650818e-01,\n",
       "        2.80785959e-01],\n",
       "       [8.16927853e-01, 8.62130474e-01, 8.09978149e-01, 3.62853025e-01,\n",
       "        5.64027955e-01],\n",
       "       [4.46676963e-01, 7.35523116e-01, 2.18018731e-01, 8.30181827e-01,\n",
       "        9.96857759e-01],\n",
       "       [6.09775992e-01, 6.38887519e-01, 4.93922459e-01, 6.26401981e-02,\n",
       "        7.67844353e-01],\n",
       "       [9.61371328e-01, 3.20948177e-01, 7.10820685e-01, 9.06482204e-01,\n",
       "        5.88213108e-01],\n",
       "       [7.31101228e-01, 8.35713168e-02, 5.24335148e-01, 6.35291972e-01,\n",
       "        7.64139651e-01],\n",
       "       [9.29014054e-01, 2.01239302e-01, 4.66438165e-01, 1.19018017e-01,\n",
       "        9.38090848e-01],\n",
       "       [8.44344161e-01, 6.72984870e-01, 6.86456848e-01, 7.46858846e-01,\n",
       "        2.19035462e-01],\n",
       "       [4.87047861e-01, 7.67651797e-01, 4.31896502e-01, 3.19250665e-02,\n",
       "        9.93763896e-01],\n",
       "       [2.66668676e-01, 2.40589975e-01, 1.17591499e-01, 6.81539350e-01,\n",
       "        8.36257322e-01],\n",
       "       [4.41594605e-01, 5.34222939e-01, 2.59617775e-01, 9.09374083e-02,\n",
       "        9.39187585e-01],\n",
       "       [4.33089663e-01, 2.54749372e-01, 8.66533318e-02, 1.19695802e-01,\n",
       "        2.53937649e-02],\n",
       "       [7.37173190e-01, 4.37332097e-01, 9.33634975e-01, 5.77122015e-01,\n",
       "        7.33339598e-01],\n",
       "       [2.57602897e-01, 8.78777555e-01, 5.04651787e-01, 5.31261110e-01,\n",
       "        1.89260766e-01],\n",
       "       [8.75804087e-01, 7.30632541e-01, 1.83404015e-02, 2.19500694e-01,\n",
       "        4.13402734e-01],\n",
       "       [2.69620590e-01, 4.89538723e-01, 1.50366441e-02, 2.67552811e-01,\n",
       "        7.45906432e-01],\n",
       "       [8.72777219e-01, 2.09624261e-03, 1.00978598e-01, 2.86169181e-01,\n",
       "        3.68526083e-01],\n",
       "       [7.51988819e-01, 3.74518345e-01, 3.59615373e-01, 8.18196714e-01,\n",
       "        7.09496062e-01],\n",
       "       [3.63382767e-01, 2.95174032e-01, 7.72166015e-01, 2.96691950e-01,\n",
       "        2.57582628e-01],\n",
       "       [2.27191232e-01, 7.88579151e-01, 2.45352195e-01, 4.46378315e-02,\n",
       "        5.18522822e-01],\n",
       "       [5.03866593e-01, 2.80586123e-01, 1.76846768e-01, 6.29099420e-01,\n",
       "        3.15825496e-02],\n",
       "       [7.05904703e-01, 5.81466997e-01, 9.92436851e-02, 2.94432339e-01,\n",
       "        1.62194994e-01],\n",
       "       [8.25857585e-01, 2.00940526e-01, 1.07299219e-01, 9.13077138e-01,\n",
       "        2.49027921e-01],\n",
       "       [8.11882312e-01, 6.44569842e-02, 1.78911923e-01, 3.38301457e-01,\n",
       "        7.82334268e-02],\n",
       "       [5.06288589e-01, 6.87460769e-02, 8.04687329e-01, 9.67441760e-01,\n",
       "        6.37229729e-01],\n",
       "       [2.78553728e-02, 5.80280467e-01, 1.68997740e-01, 2.48447017e-02,\n",
       "        5.03098354e-01],\n",
       "       [7.62355776e-01, 6.92955606e-01, 9.63326223e-02, 2.06103219e-01,\n",
       "        8.31159478e-01],\n",
       "       [8.79046703e-02, 3.80020661e-01, 1.10757354e-01, 1.99538512e-01,\n",
       "        6.89583250e-01],\n",
       "       [4.13555497e-02, 6.30497753e-02, 6.20703138e-01, 9.82831820e-01,\n",
       "        7.76544491e-01],\n",
       "       [8.14021741e-01, 9.87673785e-02, 1.26598985e-01, 6.49732187e-01,\n",
       "        5.77483442e-01],\n",
       "       [1.58576887e-01, 4.57590875e-01, 6.14656111e-01, 6.18713524e-01,\n",
       "        9.25559812e-01],\n",
       "       [2.32942112e-01, 6.06315710e-01, 7.71550337e-01, 8.61889811e-01,\n",
       "        3.35732599e-01],\n",
       "       [5.86123960e-01, 2.63649651e-01, 9.05839487e-01, 7.50048396e-01,\n",
       "        9.35550253e-01],\n",
       "       [9.10880626e-01, 6.63463851e-02, 3.26841190e-01, 1.62744891e-01,\n",
       "        8.10356821e-01],\n",
       "       [1.37941183e-01, 5.56036948e-01, 3.43321801e-01, 6.36667112e-01,\n",
       "        9.45454784e-01],\n",
       "       [3.31328607e-01, 4.14637504e-01, 5.83825726e-01, 9.08617448e-01,\n",
       "        1.48233872e-01],\n",
       "       [5.51830793e-01, 9.82647047e-01, 9.70739218e-02, 5.10709524e-02,\n",
       "        1.92081549e-01],\n",
       "       [8.18561441e-01, 6.85734209e-01, 7.51910886e-01, 2.70072488e-01,\n",
       "        5.14060509e-01],\n",
       "       [8.38805252e-01, 8.21053528e-01, 2.89700824e-01, 8.86128137e-01,\n",
       "        7.54892986e-01],\n",
       "       [9.19673191e-01, 4.77679116e-01, 7.21230025e-01, 2.19929064e-01,\n",
       "        6.20522870e-02],\n",
       "       [5.52320416e-01, 7.57887884e-02, 7.31926153e-01, 8.08804452e-01,\n",
       "        6.89204789e-01],\n",
       "       [7.59620965e-01, 8.66581353e-01, 2.04443874e-01, 8.34639561e-01,\n",
       "        5.56289439e-01],\n",
       "       [8.41262293e-01, 9.16845849e-02, 3.67357116e-01, 1.95603585e-01,\n",
       "        7.14686857e-02],\n",
       "       [8.22303171e-01, 2.04585465e-01, 5.21662564e-01, 5.39646785e-01,\n",
       "        9.97586837e-01],\n",
       "       [5.53636726e-01, 5.51594907e-01, 9.32922572e-01, 4.34364187e-01,\n",
       "        7.16419818e-01],\n",
       "       [8.90105845e-01, 9.27304644e-01, 4.50359904e-01, 4.56884486e-01,\n",
       "        4.21522770e-01]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "84d1e53956046d85",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-27T10:09:25.464563Z",
     "start_time": "2024-10-27T10:09:25.457903Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 5)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_seq.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ba1220fe2a1e3a47",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-27T10:09:03.829926Z",
     "start_time": "2024-10-27T10:09:03.573789Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "encoder = Encoder(enc_vocab_size, input_seq_length, h, d_k, d_v, d_model, d_ff, n,\n",
    "dropout_rate)\n",
    "res = encoder(input_seq, None, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e162fb9ddf672e",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Running this code produces an output of shape (batch size, sequence length, model\n",
    "dimensionality). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f57f2a6b7ab0b7b1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-27T10:09:07.934020Z",
     "start_time": "2024-10-27T10:09:07.911389Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[ 1.2713559   0.839783   -0.61490166 ...  0.4920071  -0.5204461\n",
      "   -0.82770973]\n",
      "  [ 0.5069501  -0.10065574 -0.6114606  ...  0.8215117  -0.43704084\n",
      "   -0.15034983]\n",
      "  [ 0.5849364   1.1628308  -0.9076203  ...  1.4428228  -1.2228639\n",
      "   -0.7857101 ]\n",
      "  [ 1.2106156   1.0229694  -0.95936114 ...  0.5122324  -0.9782004\n",
      "   -0.66699743]\n",
      "  [ 1.0806003   0.9080391  -0.6692018  ...  0.34345323 -0.932028\n",
      "   -0.35415187]]\n",
      "\n",
      " [[ 0.8201922   1.1076996  -0.7905818  ... -0.14174259 -1.0566348\n",
      "   -0.66603184]\n",
      "  [ 0.7477688   1.0753641  -1.002835   ...  0.5830383  -1.1875114\n",
      "   -0.6166347 ]\n",
      "  [ 1.1122088  -0.1251646  -0.6617058  ...  0.90386933 -1.1568978\n",
      "   -0.3706853 ]\n",
      "  [-0.15401462  0.5651616  -0.95096487 ...  1.1868485  -0.4684367\n",
      "   -0.15401462]\n",
      "  [ 1.164729    0.9010123  -0.40531746 ... -0.13277511 -1.6184456\n",
      "   -0.03196442]]\n",
      "\n",
      " [[ 1.3069713   1.8405747  -0.5914834  ...  1.8612355  -0.5538415\n",
      "   -0.5636145 ]\n",
      "  [ 1.444673    1.6484761  -0.8156267  ...  1.0484823  -0.6094352\n",
      "   -1.0142016 ]\n",
      "  [ 1.0179024   1.3414253   0.2651063  ...  1.3990822  -0.6574945\n",
      "   -0.67164254]\n",
      "  [ 0.8942096  -0.09570774 -0.7039316  ...  0.93599814 -0.9211742\n",
      "   -0.6547222 ]\n",
      "  [ 0.6520226   2.042537    0.57550585 ...  1.4678297  -1.0955197\n",
      "   -0.7349648 ]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[ 1.2355496   1.1974194  -0.26922974 ...  1.5306845  -1.6036999\n",
      "   -0.4812683 ]\n",
      "  [ 1.2631465   1.1244928  -0.15600325 ...  1.5121839  -1.1497089\n",
      "   -0.5889185 ]\n",
      "  [ 1.455309    0.78900826 -0.44841692 ...  1.0223328  -1.2680165\n",
      "   -1.1208669 ]\n",
      "  [ 1.338759    0.5822807  -0.5874953  ...  1.8119317  -1.2950914\n",
      "   -1.5071442 ]\n",
      "  [ 0.6634504  -0.11826174 -0.8681746  ...  1.5368633  -2.0860565\n",
      "   -0.77185845]]\n",
      "\n",
      " [[ 1.0115657   1.496615    0.15457152 ... -0.11246402  0.02684031\n",
      "   -0.66598845]\n",
      "  [ 1.4696072   1.4296777  -0.11110616 ...  0.6497206  -0.3682784\n",
      "    0.4331932 ]\n",
      "  [ 1.3473145  -0.10590719  0.03108212 ... -0.10590719  0.00723236\n",
      "   -0.37218428]\n",
      "  [-0.08634652  2.1396375   0.41989332 ...  0.50780237  0.41545945\n",
      "   -0.8050581 ]\n",
      "  [ 1.316703    0.9288066  -0.10134389 ...  0.9679239  -0.8405362\n",
      "   -0.1865546 ]]\n",
      "\n",
      " [[ 0.6710383   1.8299438  -0.96862763 ...  1.2318246  -0.4442577\n",
      "   -1.7382772 ]\n",
      "  [ 0.7218355  -0.14877021 -0.4677988  ...  1.4964211  -0.91481435\n",
      "   -0.33101955]\n",
      "  [ 0.8695886   0.7981142  -0.4604965  ...  1.3939711  -0.47271052\n",
      "   -0.24716201]\n",
      "  [-0.13581555  1.1832762  -1.0265248  ... -0.13581555 -0.23426634\n",
      "   -0.6163623 ]\n",
      "  [ 1.0187489   0.46649662 -1.5073119  ...  0.92477036 -1.2296492\n",
      "   -0.89554495]]], shape=(64, 5, 512), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "807075b4ca629c30",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-27T10:09:14.374395Z",
     "start_time": "2024-10-27T10:09:14.364155Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([64, 5, 512])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ceec9a6285e20fe",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
