{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### The Libraries"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ce250979706c3cd8"
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "outputs": [],
   "source": [
    "from pickle import load,dump,HIGHEST_PROTOCOL\n",
    "from numpy import savetxt\n",
    "from numpy.random import shuffle\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.optimizers import Adam\n",
    "from keras.optimizers.schedules import LearningRateSchedule\n",
    "from keras.metrics import Mean\n",
    "from tensorflow import data,train,convert_to_tensor, int64, math, cast,float32, linalg, ones,maximum, newaxis, reduce_sum,equal,argmax,GradientTape,TensorSpec,function,int64,Module,TensorArray\n",
    "from keras.losses import sparse_categorical_crossentropy\n",
    "from keras import Model\n",
    "from keras.layers import Dense, Input\n",
    "from time import time"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-22T04:10:23.020610Z",
     "start_time": "2025-01-22T04:10:23.016636Z"
    }
   },
   "id": "588c9e583f710274"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Playing around with the dataset"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "81d3f6ec824b8c3b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "This dataset is filled with translations of English to German. So it is essentially a neural machine translation dataset  "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "11f906f27e7f6eab"
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-01-22T04:42:40.120760Z",
     "start_time": "2025-01-22T04:42:40.083547Z"
    }
   },
   "outputs": [],
   "source": [
    "clean_dataset = load(open('english-german-both.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "outputs": [
    {
     "data": {
      "text/plain": "array([['i like both', 'ich mag beide'],\n       ['she misses him', 'er fehlt ihr'],\n       ['i followed him', 'ich folgte ihm'],\n       ...,\n       ['tom is cooking', 'tom kocht'],\n       ['youre upset', 'sie sind besturzt'],\n       ['do you see me', 'sehen sie mich']], dtype='<U370')"
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_dataset"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-22T04:42:40.488074Z",
     "start_time": "2025-01-22T04:42:40.475891Z"
    }
   },
   "id": "7a212d33b747c2be"
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "outputs": [],
   "source": [
    "sample_dataset = clean_dataset[:6000,:]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-22T04:42:41.344561Z",
     "start_time": "2025-01-22T04:42:41.338564Z"
    }
   },
   "id": "67bf84e347cbe1bd"
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "outputs": [
    {
     "data": {
      "text/plain": "array([['i like both', 'ich mag beide'],\n       ['she misses him', 'er fehlt ihr'],\n       ['i followed him', 'ich folgte ihm'],\n       ...,\n       ['open your mouth', 'offnen sie den mund'],\n       ['love is blind', 'liebe ist blind'],\n       ['go home quickly', 'geh schnell nach hause']], dtype='<U370')"
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_dataset"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-22T04:42:42.736404Z",
     "start_time": "2025-01-22T04:42:42.703836Z"
    }
   },
   "id": "96c6bb80ef360330"
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "outputs": [
    {
     "data": {
      "text/plain": "array(['<START> tom did do it <EOS>', '<START> it might rain <EOS>',\n       '<START> i was rude <EOS>', ..., '<START> im a teacher <EOS>',\n       '<START> turn it off <EOS>', '<START> i want that <EOS>'],\n      dtype='<U370')"
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_dataset[:,0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-22T05:39:59.715607Z",
     "start_time": "2025-01-22T05:39:59.689003Z"
    }
   },
   "id": "942c9b8219fa63d0"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Need to append start and end tokens for each of the sentences"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2f2b42cfebf50417"
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "outputs": [],
   "source": [
    "for i in range(sample_dataset[:,0].size):\n",
    "    sample_dataset[i, 0] = \"<START> \" + sample_dataset[i, 0] + \" <EOS>\"\n",
    "    sample_dataset[i, 1] = \"<START> \" + sample_dataset[i, 1] + \" <EOS>\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-22T04:42:44.336979Z",
     "start_time": "2025-01-22T04:42:44.326394Z"
    }
   },
   "id": "b7a6cefe936d7e74"
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "outputs": [
    {
     "data": {
      "text/plain": "array([['<START> i like both <EOS>', '<START> ich mag beide <EOS>'],\n       ['<START> she misses him <EOS>', '<START> er fehlt ihr <EOS>'],\n       ['<START> i followed him <EOS>', '<START> ich folgte ihm <EOS>'],\n       ...,\n       ['<START> open your mouth <EOS>',\n        '<START> offnen sie den mund <EOS>'],\n       ['<START> love is blind <EOS>', '<START> liebe ist blind <EOS>'],\n       ['<START> go home quickly <EOS>',\n        '<START> geh schnell nach hause <EOS>']], dtype='<U370')"
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_dataset"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-22T04:42:44.699444Z",
     "start_time": "2025-01-22T04:42:44.682048Z"
    }
   },
   "id": "f74a5b7d0abb89ea"
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "outputs": [],
   "source": [
    "shuffle(sample_dataset)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-22T04:42:45.182080Z",
     "start_time": "2025-01-22T04:42:45.179942Z"
    }
   },
   "id": "da18543b64bcbab2"
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "outputs": [
    {
     "data": {
      "text/plain": "array([['<START> tom did do it <EOS>',\n        '<START> doch tom hat es gemacht <EOS>'],\n       ['<START> it might rain <EOS>', '<START> es konnte regnen <EOS>'],\n       ['<START> i was rude <EOS>', '<START> ich war ruppig <EOS>'],\n       ...,\n       ['<START> im a teacher <EOS>', '<START> ich bin lehrerin <EOS>'],\n       ['<START> turn it off <EOS>', '<START> mach ihn aus <EOS>'],\n       ['<START> i want that <EOS>', '<START> ich will das <EOS>']],\n      dtype='<U370')"
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_dataset"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-22T04:42:45.664116Z",
     "start_time": "2025-01-22T04:42:45.651942Z"
    }
   },
   "id": "bfe303ae772d44ad"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### The task starts !!! "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e149cb0c6aa72a98"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Prepare Dataset Class"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "33ec9ebe11899902"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Creating the prepare dataset class which preprocesses and splits our dataset"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "96fbf9295560e3d"
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "outputs": [],
   "source": [
    "class PrepareDataset:\n",
    "    def __init__(self,**kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.n_sentences = 10000 # Number of sentences to include in the dataset\n",
    "        self.train_split = 0.8  # Ratio of the training data split\n",
    "        self.val_split = 0.1 # Ratio of the validation data split\n",
    "    \n",
    "    # Fit a tokenizer\n",
    "    def create_tokenizer(self, dataset):\n",
    "        tokenizer = Tokenizer()\n",
    "        tokenizer.fit_on_texts(dataset)\n",
    "        return tokenizer\n",
    "    \n",
    "    def find_seq_length(self, dataset):\n",
    "        return max(len(seq.split()) for seq in dataset)\n",
    "    \n",
    "    def find_vocab_size(self, tokenizer, dataset):\n",
    "        tokenizer.fit_on_texts(dataset)\n",
    "        return len(tokenizer.word_index) + 1\n",
    "\n",
    "    # Encode and pad the input sequences\n",
    "    def encode_pad(self, dataset, tokenizer, seq_length):\n",
    "        x = tokenizer.texts_to_sequences(dataset)\n",
    "        x = pad_sequences(x, maxlen=seq_length, padding='post')\n",
    "        x = convert_to_tensor(x, dtype=int64)\n",
    "        return x\n",
    "    \n",
    "    def save_tokenizer(self, tokenizer, name):\n",
    "        with open(name + '_tokenizer.pkl', 'wb') as handle:\n",
    "            dump(tokenizer, handle, protocol=HIGHEST_PROTOCOL)\n",
    "    \n",
    "    def __call__(self, filename, **kwargs):\n",
    "        # Load a clean dataset\n",
    "        clean_dataset = load(open(filename, 'rb'))\n",
    "        \n",
    "        # Reduce dataset size\n",
    "        dataset = clean_dataset[:self.n_sentences,:]\n",
    "        \n",
    "        # Including the start and end tokens\n",
    "        for i in range(dataset[:,0].size):\n",
    "            dataset[i, 0] = \"<START> \" + dataset[i, 0] + \" <EOS>\"\n",
    "            dataset[i, 1] = \"<START> \" + dataset[i, 1] + \" <EOS>\"\n",
    "        \n",
    "        # Random shuffle\n",
    "        shuffle(dataset)\n",
    "        \n",
    "        # Split the dataset\n",
    "        train = dataset[:int(self.n_sentences * self.train_split)]\n",
    "        val = dataset[int(self.n_sentences * self.train_split):int(self.n_sentences * (1-self.val_split))]\n",
    "        test = dataset[int(self.n_sentences * (1 - self.val_split)):]\n",
    "        \n",
    "        # Prepare tokenizer for the encoder input\n",
    "        enc_tokenizer = self.create_tokenizer(dataset[:,0])\n",
    "        enc_seq_length = self.find_seq_length(dataset[:,0])\n",
    "        enc_vocab_size = self.find_vocab_size(enc_tokenizer, train[:, 0])\n",
    "        \n",
    "         # Prepare tokenizer for the decoder input\n",
    "        dec_tokenizer = self.create_tokenizer(dataset[:, 1])\n",
    "        dec_seq_length = self.find_seq_length(dataset[:, 1])\n",
    "        dec_vocab_size = self.find_vocab_size(dec_tokenizer, train[:, 1])\n",
    "        \n",
    "        # Encode and pad the input sequences\n",
    "        trainX = self.encode_pad(train[:, 0], enc_tokenizer, enc_seq_length)\n",
    "        trainY = self.encode_pad(train[:, 1], dec_tokenizer, dec_seq_length)\n",
    "        \n",
    "        \n",
    "        # Encode and pad the validation input\n",
    "        valX = self.encode_pad(val[:, 0], enc_tokenizer, enc_seq_length)\n",
    "        valY = self.encode_pad(val[:, 1], dec_tokenizer, dec_seq_length)\n",
    "        \n",
    "        \n",
    "        # Save the encoder tokenizer\n",
    "        self.save_tokenizer(enc_tokenizer, 'enc')\n",
    "        # Save the decoder tokenizer\n",
    "        self.save_tokenizer(dec_tokenizer, 'dec')\n",
    "        # Save the testing dataset into a text file\n",
    "        savetxt('test_dataset.txt', test, fmt='%s')\n",
    "    \n",
    "        return (trainX, trainY,valX,valY, train,val, enc_seq_length, dec_seq_length,enc_vocab_size, dec_vocab_size)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-22T04:56:28.559927Z",
     "start_time": "2025-01-22T04:56:28.551701Z"
    }
   },
   "id": "9c359e337a4bcec1"
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "outputs": [],
   "source": [
    "dataset = PrepareDataset()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-22T04:56:29.013777Z",
     "start_time": "2025-01-22T04:56:29.007719Z"
    }
   },
   "id": "7e9f43342548fa9b"
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<START> she needs it <EOS> \n",
      " tf.Tensor([  1  26 273   6   2   0   0], shape=(7,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "trainX, trainY, ValX,ValY,train_orig,Val_orig, enc_seq_length, dec_seq_length,enc_vocab_size, dec_vocab_size = dataset('english-german-both.pkl')\n",
    "print(train_orig[0, 0], '\\n', trainX[0, :])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-22T04:56:29.800013Z",
     "start_time": "2025-01-22T04:56:29.427530Z"
    }
   },
   "id": "4a84aba87ace8868"
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "outputs": [
    {
     "data": {
      "text/plain": "2404"
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_vocab_size"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-22T04:56:30.327857Z",
     "start_time": "2025-01-22T04:56:30.321564Z"
    }
   },
   "id": "874c27f90540e8a2"
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "outputs": [
    {
     "data": {
      "text/plain": "3864"
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dec_vocab_size"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-22T04:56:31.307858Z",
     "start_time": "2025-01-22T04:56:31.301353Z"
    }
   },
   "id": "9a41941083556852"
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "outputs": [
    {
     "data": {
      "text/plain": "7"
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_seq_length"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-22T04:56:34.301474Z",
     "start_time": "2025-01-22T04:56:34.286381Z"
    }
   },
   "id": "50404633f8a44050"
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "outputs": [
    {
     "data": {
      "text/plain": "12"
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dec_seq_length"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-22T04:56:34.773084Z",
     "start_time": "2025-01-22T04:56:34.766910Z"
    }
   },
   "id": "8583e218469c03cd"
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "outputs": [
    {
     "data": {
      "text/plain": "array([['<START> good for you <EOS>', '<START> schon fur sie <EOS>'],\n       ['<START> toms thirsty <EOS>', '<START> tom hat durst <EOS>'],\n       ['<START> tom will cook <EOS>', '<START> tom wird kochen <EOS>'],\n       ...,\n       ['<START> cut the engine <EOS>',\n        '<START> mach den motor aus <EOS>'],\n       ['<START> toms great <EOS>', '<START> tom ist toll <EOS>'],\n       ['<START> im very cold <EOS>', '<START> mir ist sehr kalt <EOS>']],\n      dtype='<U370')"
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_orig"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-22T04:02:45.956798Z",
     "start_time": "2025-01-22T04:02:45.951022Z"
    }
   },
   "id": "1da978fdc2377b4f"
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: shape=(8000, 7), dtype=int64, numpy=\narray([[  1, 143,  78, ...,   2,   0,   0],\n       [  1,  45, 347, ...,   0,   0,   0],\n       [  1,   4,  65, ...,   2,   0,   0],\n       ...,\n       [  1, 239,  12, ...,   2,   0,   0],\n       [  1,  45, 319, ...,   0,   0,   0],\n       [  1,  10,  86, ...,   2,   0,   0]])>"
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainX"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-22T04:02:45.964185Z",
     "start_time": "2025-01-22T04:02:45.955515Z"
    }
   },
   "id": "5ab36be200214f92"
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder sequence length: 7\n"
     ]
    }
   ],
   "source": [
    "print('Encoder sequence length:', enc_seq_length)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-22T04:02:45.964417Z",
     "start_time": "2025-01-22T04:02:45.959050Z"
    }
   },
   "id": "4e2be2a40b516d4a"
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<START> schon fur sie <EOS> \n",
      " tf.Tensor([ 1 85 78  6  2  0  0  0  0  0], shape=(10,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "print(train_orig[0, 1], '\\n', trainY[0, :])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-22T04:02:45.964522Z",
     "start_time": "2025-01-22T04:02:45.961900Z"
    }
   },
   "id": "56aa1f7144230087"
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder sequence length: 10\n"
     ]
    }
   ],
   "source": [
    "print('Decoder sequence length:', dec_seq_length)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-22T04:02:45.970181Z",
     "start_time": "2025-01-22T04:02:45.965015Z"
    }
   },
   "id": "70b24fa68bc09497"
  },
  {
   "cell_type": "markdown",
   "source": [
    "We must apply a padding value to make sure that the zero values we have appended are not processed incorrectly"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6f77b3ff54a9333a"
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "outputs": [],
   "source": [
    "def padding_mask(input):\n",
    "    # Create mask which marks the zero padding values in the input by a 1\n",
    "    mask = math.equal(input, 0)\n",
    "    mask = cast(mask, float32)\n",
    "    return mask"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-22T04:02:45.970258Z",
     "start_time": "2025-01-22T04:02:45.967460Z"
    }
   },
   "id": "bfbdce9edde687c2"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Upon receiving an input, this function will generate a tensor that marks by a value of one\n",
    "wherever the input contains a value of zero."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d0562bd6c0a06e7e"
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([0. 0. 0. 0. 1. 1. 1.], shape=(7,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "from numpy import array\n",
    "input = array([1, 2, 3, 4, 0, 0, 0])\n",
    "print(padding_mask(input))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-22T04:02:46.020601Z",
     "start_time": "2025-01-22T04:02:45.969237Z"
    }
   },
   "id": "60d558b0d16f7428"
  },
  {
   "cell_type": "markdown",
   "source": [
    "A look-ahead mask is required to prevent the decoder from attending to succeeding words,\n",
    "such that the prediction for a particular word can only depend on known outputs for the\n",
    "words that come before it."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fff2502071aee8d9"
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "outputs": [],
   "source": [
    "def lookahead_mask(shape):\n",
    "    # Mask out future entries by marking them with a 1.0\n",
    "    mask = 1 - linalg.band_part(ones((shape, shape)), -1, 0)\n",
    "    return mask"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-22T04:02:46.020735Z",
     "start_time": "2025-01-22T04:02:45.994391Z"
    }
   },
   "id": "f846e65a75c2d2f1"
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0. 1. 1. 1. 1.]\n",
      " [0. 0. 1. 1. 1.]\n",
      " [0. 0. 0. 1. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]], shape=(5, 5), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(lookahead_mask(5))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-22T04:02:46.021217Z",
     "start_time": "2025-01-22T04:02:45.997020Z"
    }
   },
   "id": "327c0891e642831a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Joining the transformer encoder and decoder module"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "14bd8dd7a595e80a"
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import random\n",
    "import numpy as np\n",
    "from keras.layers import LayerNormalization, Layer, Dense, ReLU, Dropout,TextVectorization, Embedding\n",
    "from keras.backend import softmax \n",
    "from tensorflow import math, matmul, reshape, shape, transpose, cast, float32"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-22T04:02:46.021273Z",
     "start_time": "2025-01-22T04:02:46.009926Z"
    }
   },
   "id": "359bb4b97dc44feb"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### General Modules - Positional embedding, attention, Feed forward layer, Normalization"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "452922ee585b5024"
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "outputs": [],
   "source": [
    "class PositionEmbeddingFixedWeights(Layer):\n",
    "    def __init__(self, seq_length, vocab_size, output_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        word_embedding_matrix = self.get_position_encoding(vocab_size, output_dim)\n",
    "        pos_embedding_matrix = self.get_position_encoding(seq_length, output_dim)\n",
    "        self.word_embedding_layer = Embedding(input_dim=vocab_size, output_dim=output_dim,weights=[word_embedding_matrix],trainable=False)\n",
    "        self.position_embedding_layer = Embedding(input_dim=seq_length, output_dim=output_dim,weights=[pos_embedding_matrix],trainable=False)\n",
    "    \n",
    "    def get_position_encoding(self, seq_len, d, n=10000):\n",
    "        P = np.zeros((seq_len, d))\n",
    "        for k in range(seq_len):\n",
    "            for i in np.arange(int(d/2)):\n",
    "                denominator = np.power(n, 2*i/d)\n",
    "                P[k, 2*i] = np.sin(k/denominator)\n",
    "                P[k, 2*i+1] = np.cos(k/denominator)\n",
    "        return P\n",
    "    \n",
    "    \n",
    "    def call(self, inputs):\n",
    "        position_indices = tf.range(tf.shape(inputs)[-1])\n",
    "        embedded_words = self.word_embedding_layer(inputs)\n",
    "        embedded_indices = self.position_embedding_layer(position_indices)\n",
    "        return embedded_words + embedded_indices"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-22T04:02:46.022358Z",
     "start_time": "2025-01-22T04:02:46.014811Z"
    }
   },
   "id": "dfca1f179bfad291"
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "outputs": [],
   "source": [
    "class DotProductAttention(Layer):\n",
    "    def __init__(self,**kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "    \n",
    "    def call(self,queries,keys,values,d_k,mask = None):\n",
    "        \n",
    "        # Scoring the queries against the keys after transposing the latter, and scaling\n",
    "        scores = matmul(queries, keys, transpose_b=True) / math.sqrt(cast(d_k, float32))\n",
    "        \n",
    "        # Apply mask to the attention scores\n",
    "        if mask is not None:\n",
    "            scores += -1e9 * mask\n",
    "        \n",
    "        # Computing the weights by a softmax operation\n",
    "        weights = softmax(scores)\n",
    "        \n",
    "        # Computing the attention by a weighted sum of the value vectors\n",
    "        return matmul(weights, values)\n",
    "        \n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-22T04:02:46.022451Z",
     "start_time": "2025-01-22T04:02:46.018187Z"
    }
   },
   "id": "b41de95a9262ff46"
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "outputs": [],
   "source": [
    "class MultiHeadAttention(Layer):\n",
    "    def __init__(self, h, d_k, d_v, d_model, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.attention = DotProductAttention() # Scaled dot product attention\n",
    "        self.heads = h # Number of attention heads to use\n",
    "        self.d_k = d_k # Dimensionality of the linearly projected queries and keys\n",
    "        self.d_v = d_v # Dimensionality of the linearly projected values\n",
    "        self.d_model = d_model # Dimensionality of the model\n",
    "        self.W_q = Dense(d_k) # Learned projection matrix for the queries\n",
    "        self.W_k = Dense(d_k) # Learned projection matrix for the keys\n",
    "        self.W_v = Dense(d_v) # Learned projection matrix for the values\n",
    "        self.W_o = Dense(d_model) # Learned projection matrix for the multi-head output\n",
    "    def reshape_tensor(self, x, heads, flag):\n",
    "        if flag:\n",
    "            # Tensor shape after reshaping and transposing:\n",
    "            # (batch_size, heads, seq_length, -1)\n",
    "            x = reshape(x, shape=(shape(x)[0], shape(x)[1], heads, -1))\n",
    "            x = transpose(x, perm=(0, 2, 1, 3))\n",
    "        else:\n",
    "            # Reverting the reshaping and transposing operations:\n",
    "            # (batch_size, seq_length, d_k)\n",
    "            x = transpose(x, perm=(0, 2, 1, 3))\n",
    "            x = reshape(x, shape=(shape(x)[0], shape(x)[1], self.d_k))\n",
    "        return x\n",
    "    \n",
    "    def call(self, queries, keys, values, mask=None):\n",
    "        # Rearrange the queries to be able to compute all heads in parallel\n",
    "        q_reshaped = self.reshape_tensor(self.W_q(queries), self.heads, True)\n",
    "        # Resulting tensor shape: (batch_size, heads, input_seq_length, -1)\n",
    "        # Rearrange the keys to be able to compute all heads in parallel\n",
    "        k_reshaped = self.reshape_tensor(self.W_k(keys), self.heads, True)\n",
    "        # Resulting tensor shape: (batch_size, heads, input_seq_length, -1)\n",
    "        # Rearrange the values to be able to compute all heads in parallel\n",
    "        v_reshaped = self.reshape_tensor(self.W_v(values), self.heads, True)\n",
    "        # Resulting tensor shape: (batch_size, heads, input_seq_length, -1)\n",
    "        # Compute the multi-head attention output using the reshaped queries,\n",
    "        # keys, and values\n",
    "        o_reshaped = self.attention(q_reshaped, k_reshaped, v_reshaped, self.d_k, mask)\n",
    "        # Resulting tensor shape: (batch_size, heads, input_seq_length, -1)\n",
    "        # Rearrange back the output into concatenated form\n",
    "        output = self.reshape_tensor(o_reshaped, self.heads, False)\n",
    "        # Resulting tensor shape: (batch_size, input_seq_length, d_v)\n",
    "        # Apply one final linear projection to the output to generate the multi-head\n",
    "        # attention. Resulting tensor shape: (batch_size, input_seq_length, d_model)\n",
    "        return self.W_o(output)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-22T04:02:46.034412Z",
     "start_time": "2025-01-22T04:02:46.022985Z"
    }
   },
   "id": "b9b32dc6b6adad2b"
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "outputs": [],
   "source": [
    "# Implementing the Add & Norm Layer\n",
    "class AddNormalization(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.layer_norm = LayerNormalization() # Layer normalization layer\n",
    "    def call(self, x, sublayer_x):\n",
    "        # The sublayer input and output need to be of the same shape to be summed\n",
    "        add = x + sublayer_x\n",
    "        # Apply layer normalization to the sum\n",
    "        return self.layer_norm(add)\n",
    "\n",
    "# Implementing the Feed-Forward Layer\n",
    "class FeedForward(Layer):\n",
    "    def __init__(self, d_ff, d_model, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.fully_connected1 = Dense(d_ff) # First fully connected layer\n",
    "        self.fully_connected2 = Dense(d_model) # Second fully connected layer\n",
    "        self.activation = ReLU() # ReLU activation layer\n",
    "        \n",
    "    def call(self, x):\n",
    "        # The input is passed into the two fully-connected layers, with a ReLU in between\n",
    "        x_fc1 = self.fully_connected1(x)\n",
    "        return self.fully_connected2(self.activation(x_fc1))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-22T04:02:46.034545Z",
     "start_time": "2025-01-22T04:02:46.029349Z"
    }
   },
   "id": "cf934f40f7c0de71"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Encoder Modules"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "45c774a8e69296fc"
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "outputs": [],
   "source": [
    "# Implementing the Encoder Layer\n",
    "class EncoderLayer(Layer):\n",
    "    def __init__(self,sequence_length, h, d_k, d_v, d_model, d_ff, rate, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.build(input_shape=[None, sequence_length, d_model])\n",
    "        self.d_model = d_model\n",
    "        self.sequence_length = sequence_length\n",
    "        self.multihead_attention = MultiHeadAttention(h, d_k, d_v, d_model)\n",
    "        self.dropout1 = Dropout(rate)\n",
    "        self.add_norm1 = AddNormalization()\n",
    "        self.feed_forward = FeedForward(d_ff, d_model)\n",
    "        self.dropout2 = Dropout(rate)\n",
    "        self.add_norm2 = AddNormalization()\n",
    "        \n",
    "    \n",
    "    def build_graph(self):\n",
    "        input_layer = Input(shape=(self.sequence_length, self.d_model))\n",
    "        return Model(inputs=[input_layer], outputs=self.call(input_layer, None, True))\n",
    "    \n",
    "    \n",
    "    def call(self, x, padding_mask, training):\n",
    "        # Multi-head attention layer\n",
    "        multihead_output = self.multihead_attention(x, x, x, padding_mask)\n",
    "        \n",
    "        # Expected output shape = (batch_size, sequence_length, d_model)\n",
    "        # Add in a dropout layer\n",
    "        multihead_output = self.dropout1(multihead_output, training=training)\n",
    "        # Followed by an Add & Norm layer\n",
    "        addnorm_output = self.add_norm1(x, multihead_output)\n",
    "        # Expected output shape = (batch_size, sequence_length, d_model)\n",
    "        # Followed by a fully connected layer\n",
    "        feedforward_output = self.feed_forward(addnorm_output)\n",
    "        # Expected output shape = (batch_size, sequence_length, d_model)\n",
    "        # Add in another dropout layer\n",
    "        feedforward_output = self.dropout2(feedforward_output, training=training)\n",
    "        # Followed by another Add & Norm layer\n",
    "        return self.add_norm2(addnorm_output, feedforward_output)\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-22T04:02:46.035211Z",
     "start_time": "2025-01-22T04:02:46.030753Z"
    }
   },
   "id": "75d99d601f3c90de"
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "outputs": [],
   "source": [
    "# Implementing the Encoder\n",
    "class Encoder(Layer):\n",
    "    def __init__(self, vocab_size, sequence_length, h, d_k, d_v, d_model, d_ff, n, rate,\n",
    "    **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.pos_encoding = PositionEmbeddingFixedWeights(sequence_length, vocab_size,\n",
    "        d_model)\n",
    "        self.dropout = Dropout(rate)\n",
    "        self.encoder_layer = [EncoderLayer(sequence_length,h, d_k, d_v, d_model, d_ff, rate) for _ in range(n)]\n",
    "    \n",
    "  \n",
    "    def call(self, input_sentence, padding_mask, training):\n",
    "        # Generate the positional encoding\n",
    "        pos_encoding_output = self.pos_encoding(input_sentence)\n",
    "        # Expected output shape = (batch_size, sequence_length, d_model)\n",
    "        # Add in a dropout layer\n",
    "        x = self.dropout(pos_encoding_output, training=training)\n",
    "        # Pass on the positional encoded values to each encoder layer\n",
    "        for i, layer in enumerate(self.encoder_layer):\n",
    "            x = layer(x, padding_mask, training)\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-22T04:02:46.078817Z",
     "start_time": "2025-01-22T04:02:46.036844Z"
    }
   },
   "id": "2a0cda2d2205f04d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Decoder Modules"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "aac08602889ea1e4"
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "outputs": [],
   "source": [
    "class DecoderLayer(Layer):\n",
    "    def __init__(self,sequence_length, h, d_k, d_v, d_model, d_ff, rate, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.build(input_shape=[None, sequence_length, d_model])\n",
    "        self.d_model = d_model\n",
    "        self.sequence_length = sequence_length\n",
    "        self.multihead_attention1 = MultiHeadAttention(h, d_k, d_v, d_model)\n",
    "        self.dropout1 = Dropout(rate)\n",
    "        self.add_norm1 = AddNormalization()\n",
    "        self.multihead_attention2 = MultiHeadAttention(h, d_k, d_v, d_model)\n",
    "        self.dropout2 = Dropout(rate)\n",
    "        self.add_norm2 = AddNormalization()\n",
    "        self.feed_forward = FeedForward(d_ff, d_model)\n",
    "        self.dropout3 = Dropout(rate)\n",
    "        self.add_norm3 = AddNormalization()\n",
    "        \n",
    "    \n",
    "    def build_graph(self):\n",
    "        input_layer = Input(shape=(self.sequence_length, self.d_model))\n",
    "        return Model(inputs=[input_layer],outputs=self.call(input_layer, input_layer, None, None, True))\n",
    "    \n",
    "    \n",
    "    def call(self, x, encoder_output, lookahead_mask, padding_mask, training):\n",
    "        # Multi-head attention layer\n",
    "        multihead_output1 = self.multihead_attention1(x, x, x, lookahead_mask)\n",
    "         \n",
    "        # Expected output shape = (batch_size, sequence_length, d_model)\n",
    "        # Add in a dropout layer\n",
    "        multihead_output1 = self.dropout1(multihead_output1, training=training)\n",
    "        # Followed by an Add & Norm layer\n",
    "        addnorm_output1 = self.add_norm1(x, multihead_output1)\n",
    "        # Expected output shape = (batch_size, sequence_length, d_model)\n",
    "        # Followed by another multi-head attention layer\n",
    "        multihead_output2 = self.multihead_attention2(addnorm_output1, encoder_output,encoder_output, padding_mask)\n",
    "         \n",
    "        # Add in another dropout layer\n",
    "        multihead_output2 = self.dropout2(multihead_output2, training=training)\n",
    "        \n",
    "        # Followed by another Add & Norm layer\n",
    "        addnorm_output2 = self.add_norm1(addnorm_output1, multihead_output2)\n",
    "        \n",
    "        # Followed by a fully connected layer\n",
    "        feedforward_output = self.feed_forward(addnorm_output2)\n",
    "        # Expected output shape = (batch_size, sequence_length, d_model)\n",
    "        \n",
    "        # Add in another dropout layer\n",
    "        feedforward_output = self.dropout3(feedforward_output, training=training)\n",
    "        \n",
    "        # Followed by another Add & Norm layer\n",
    "        return self.add_norm3(addnorm_output2, feedforward_output)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-22T04:02:46.079632Z",
     "start_time": "2025-01-22T04:02:46.041383Z"
    }
   },
   "id": "406ec55b72117f0"
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "outputs": [],
   "source": [
    "class Decoder(Layer):\n",
    "    def __init__(self, vocab_size, sequence_length, h, d_k, d_v, d_model, d_ff, n, rate,**kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.pos_encoding = PositionEmbeddingFixedWeights(sequence_length, vocab_size,d_model)\n",
    "        self.dropout = Dropout(rate)\n",
    "        self.decoder_layer = [DecoderLayer(sequence_length,h, d_k, d_v, d_model, d_ff, rate) for _ in range(n)]\n",
    "        \n",
    "    \n",
    "    def call(self, output_target, encoder_output, lookahead_mask, padding_mask, training):\n",
    "        # Generate the positional encoding\n",
    "        pos_encoding_output = self.pos_encoding(output_target)\n",
    "        # Expected output shape = (number of sentences, sequence_length, d_model)\n",
    "        # Add in a dropout layer\n",
    "        x = self.dropout(pos_encoding_output, training=training)\n",
    "        # Pass on the positional encoded values to each encoder layer\n",
    "        for i, layer in enumerate(self.decoder_layer):\n",
    "            x = layer(x, encoder_output, lookahead_mask, padding_mask, training)\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-22T04:02:46.079853Z",
     "start_time": "2025-01-22T04:02:46.045123Z"
    }
   },
   "id": "1bf09e842149c38d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Creating the transformer by joining the Encoder and Decoder"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fe7d2bcb004c1122"
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "outputs": [],
   "source": [
    "class TransformerModel(Model):\n",
    "    def __init__(self, enc_vocab_size, dec_vocab_size, enc_seq_length, dec_seq_length, h, d_k, d_v, d_model, d_ff_inner, n, rate, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        # Set up the encoder\n",
    "        self.encoder = Encoder(enc_vocab_size, enc_seq_length, h, d_k, d_v,d_model, d_ff_inner, n, rate)\n",
    "        \n",
    "        # Set up the decoder\n",
    "        self.decoder = Decoder(dec_vocab_size, dec_seq_length, h, d_k, d_v,d_model, d_ff_inner, n, rate)\n",
    "        \n",
    "        # Define the final dense layer\n",
    "        self.model_last_layer = Dense(dec_vocab_size)\n",
    "    \n",
    "    def padding_mask(self, input):\n",
    "        # Create mask which marks the zero padding values in the input by a 1.0\n",
    "        mask = math.equal(input, 0)\n",
    "        mask = cast(mask, float32)\n",
    "        # The shape of the mask should be broadcastable to the shape\n",
    "        # of the attention weights that it will be masking later on\n",
    "        return mask[:, newaxis, newaxis, :]\n",
    "\n",
    "    def lookahead_mask(self,shape):\n",
    "        # Mask out future entries by marking them with a 1.0\n",
    "        mask = 1 - linalg.band_part(ones((shape, shape)), -1, 0)\n",
    "        return mask\n",
    "\n",
    "    def call(self, encoder_input, decoder_input, training):\n",
    "        # Create padding mask to mask the encoder inputs and the encoder\n",
    "        # outputs in the decoder\n",
    "        enc_padding_mask = self.padding_mask(encoder_input)\n",
    "        \n",
    "        # Create and combine padding and look-ahead masks to be fed into the decoder\n",
    "        dec_in_padding_mask = self.padding_mask(decoder_input)\n",
    "        dec_in_lookahead_mask = self.lookahead_mask(decoder_input.shape[1])\n",
    "        dec_in_lookahead_mask = maximum(dec_in_padding_mask, dec_in_lookahead_mask)\n",
    "        \n",
    "        # Feed the input into the encoder\n",
    "        encoder_output = self.encoder(encoder_input, enc_padding_mask, training)\n",
    "        \n",
    "        # Feed the encoder output into the decoder\n",
    "        decoder_output = self.decoder(decoder_input, encoder_output,dec_in_lookahead_mask, enc_padding_mask, training)\n",
    "        \n",
    "        # Pass the decoder output through a final dense layer\n",
    "        model_output = self.model_last_layer(decoder_output)\n",
    "        \n",
    "        return model_output\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-22T04:02:46.079953Z",
     "start_time": "2025-01-22T04:02:46.049539Z"
    }
   },
   "id": "3b46159c5d10595"
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "outputs": [],
   "source": [
    "# Define model parameters\n",
    "h = 8 # Number of self-attention heads\n",
    "d_k = 64 # Dimensionality of the linearly projected queries and keys\n",
    "d_v = 64 # Dimensionality of the linearly projected values\n",
    "d_ff = 2048 # Dimensionality of the inner fully connected layer\n",
    "d_model = 512 # Dimensionality of the model sub-layers' outputs\n",
    "n = 6 # Number of layers in the encoder and decoder stack\n",
    "\n",
    "\n",
    "# Defining the training parameters\n",
    "epochs = 20\n",
    "batch_size = 64\n",
    "beta_1 = 0.9\n",
    "beta_2 = 0.98\n",
    "epsilon = 1e-9\n",
    "dropout_rate = 0.1 "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-22T04:02:46.080016Z",
     "start_time": "2025-01-22T04:02:46.052006Z"
    }
   },
   "id": "b618cbf808dfc90c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "We also need to implement a learning rate scheduler that initially increases the learning\n",
    "rate linearly for the first warmup_steps and then decreases it proportionally to the inverse\n",
    "square root of the step number."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "324bac2d0e74e3d8"
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "outputs": [],
   "source": [
    "# Implementing a learning rate scheduler\n",
    "class LRScheduler(LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.d_model = cast(d_model, float32)\n",
    "        self.warmup_steps = warmup_steps\n",
    "    def __call__(self, step_num):\n",
    "        # Linearly increasing the learning rate for the first warmup_steps, and\n",
    "        # decreasing it thereafter\n",
    "        \n",
    "        step_num = cast(step_num, float32)  # Cast step_num to float32\n",
    "        arg1 = step_num ** -0.5\n",
    "        arg2 = step_num * (self.warmup_steps ** -1.5)\n",
    "        return (self.d_model ** -0.5) * math.minimum(arg1, arg2)\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-22T04:02:46.080350Z",
     "start_time": "2025-01-22T04:02:46.055839Z"
    }
   },
   "id": "2268910917ec9499"
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    }
   ],
   "source": [
    "# Instantiate an Adam optimizer\n",
    "optimizer = Adam(LRScheduler(d_model), beta_1, beta_2, epsilon)\n",
    "# Prepare the training data\n",
    "dataset = PrepareDataset()\n",
    "trainX, trainY,valX,valY, train_orig,val_orig, enc_seq_length, dec_seq_length, enc_vocab_size, dec_vocab_size = dataset('english-german-both.pkl')\n",
    "\n",
    "# Prepare the dataset batches\n",
    "train_dataset = data.Dataset.from_tensor_slices((trainX, trainY))\n",
    "train_dataset = train_dataset.batch(batch_size)\n",
    "\n",
    "# Prepare the validation dataset batches\n",
    "val_dataset = data.Dataset.from_tensor_slices((valX, valY))\n",
    "val_dataset = val_dataset.batch(batch_size)\n",
    "\n",
    "# Create model\n",
    "training_model = TransformerModel(enc_vocab_size, dec_vocab_size, enc_seq_length,dec_seq_length, h, d_k, d_v, d_model, d_ff, n, dropout_rate)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-22T04:02:49.543426Z",
     "start_time": "2025-01-22T04:02:46.058443Z"
    }
   },
   "id": "dd3116102d060254"
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "outputs": [],
   "source": [
    "def loss_fcn(target, prediction):\n",
    "    \n",
    "    # Create mask so that the zero padding values are not included in the\n",
    "    # computation of loss\n",
    "    mask = math.logical_not(equal(target, 0))\n",
    "    mask = cast(mask, float32)\n",
    "    # Compute a sparse categorical cross-entropy loss on the unmasked values\n",
    "    loss = sparse_categorical_crossentropy(target, prediction, from_logits=True) * mask\n",
    "    # Compute the mean loss over the unmasked values\n",
    "    return reduce_sum(loss) / reduce_sum(mask)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-22T04:02:49.545336Z",
     "start_time": "2025-01-22T04:02:49.542843Z"
    }
   },
   "id": "699c522c6aac326d"
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "outputs": [],
   "source": [
    "def accuracy_fcn(target, prediction):\n",
    "    # Create mask so that the zero padding values are not included in the\n",
    "    # computation of accuracy\n",
    "    mask = math.logical_not(math.equal(target, 0))\n",
    "    # Find equal prediction and target values, and apply the padding mask\n",
    "    accuracy = equal(target, argmax(prediction, axis=2))\n",
    "    accuracy = math.logical_and(mask, accuracy)\n",
    "    # Cast the True/False values to 32-bit-precision floating-point numbers\n",
    "    mask = cast(mask, float32)\n",
    "    accuracy = cast(accuracy, float32)\n",
    "    # Compute the mean accuracy over the unmasked values\n",
    "    return reduce_sum(accuracy) / reduce_sum(mask)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-22T04:02:49.547666Z",
     "start_time": "2025-01-22T04:02:49.545560Z"
    }
   },
   "id": "e7966d92e9dfd498"
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "outputs": [],
   "source": [
    "# Include metrics monitoring\n",
    "train_loss = Mean(name='train_loss')\n",
    "train_accuracy = Mean(name='train_accuracy')\n",
    "val_loss = Mean(name='val_loss')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-22T04:02:49.559877Z",
     "start_time": "2025-01-22T04:02:49.549628Z"
    }
   },
   "id": "e9b1ed51a0234701"
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "outputs": [],
   "source": [
    "# Create a checkpoint object and manager to manage multiple checkpoints\n",
    "ckpt = train.Checkpoint(model=training_model, optimizer=optimizer)\n",
    "ckpt_manager = train.CheckpointManager(ckpt, \"./checkpoints\", max_to_keep=3)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-22T04:02:49.581643Z",
     "start_time": "2025-01-22T04:02:49.561207Z"
    }
   },
   "id": "f30c3537b6b9cd00"
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "outputs": [],
   "source": [
    "# Initialise dictionaries to store the training and validation losses\n",
    "train_loss_dict = {}\n",
    "val_loss_dict = {}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-22T04:02:49.582730Z",
     "start_time": "2025-01-22T04:02:49.571053Z"
    }
   },
   "id": "5bb73deb2abf20d7"
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "outputs": [],
   "source": [
    "# Speeding up the training process\n",
    "@function\n",
    "def train_step(encoder_input, decoder_input, decoder_output):\n",
    "    with GradientTape() as tape:\n",
    "        # Run the forward pass of the model to generate a prediction\n",
    "        prediction = training_model(encoder_input, decoder_input, training=True)\n",
    "        # Compute the training loss\n",
    "        loss = loss_fcn(decoder_output, prediction)\n",
    "        # Compute the training accuracy\n",
    "        accuracy = accuracy_fcn(decoder_output, prediction)\n",
    "    # Retrieve gradients of the trainable variables with respect to the training loss\n",
    "    gradients = tape.gradient(loss, training_model.trainable_weights)\n",
    "    # Update the values of the trainable variables by gradient descent\n",
    "    optimizer.apply_gradients(zip(gradients, training_model.trainable_weights))\n",
    "    train_loss(loss)\n",
    "    train_accuracy(accuracy)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-22T04:02:50.298286Z",
     "start_time": "2025-01-22T04:02:50.297231Z"
    }
   },
   "id": "525b6fd616e9c0a7"
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Start of epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-22 09:32:56.885537: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:961] model_pruner failed: INVALID_ARGUMENT: Graph does not contain terminal node Adam/AssignAddVariableOp.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Step 0 Loss 8.2049Accuracy 0.0000\n",
      "Epoch 1 Step 50 Loss 7.4772Accuracy 0.1461\n",
      "Epoch 1 Step 100 Loss 6.8976Accuracy 0.1830\n",
      "Epoch 1: Training Loss 6.6855, Training Accuracy 0.1986, Validation Loss 5.4345\n",
      "Saved checkpoint at epoch 1\n",
      "\n",
      "Start of epoch 2\n",
      "Epoch 2 Step 0 Loss 5.6240Accuracy 0.2947\n",
      "Epoch 2 Step 50 Loss 5.4802Accuracy 0.2790\n",
      "Epoch 2 Step 100 Loss 5.2994Accuracy 0.2873\n",
      "Epoch 2: Training Loss 5.2045, Training Accuracy 0.2925, Validation Loss 4.5735\n",
      "Saved checkpoint at epoch 2\n",
      "\n",
      "Start of epoch 3\n",
      "Epoch 3 Step 0 Loss 4.6632Accuracy 0.3298\n",
      "Epoch 3 Step 50 Loss 4.6694Accuracy 0.3296\n",
      "Epoch 3 Step 100 Loss 4.5363Accuracy 0.3450\n",
      "Epoch 3: Training Loss 4.4632, Training Accuracy 0.3541, Validation Loss 4.0538\n",
      "Saved checkpoint at epoch 3\n",
      "\n",
      "Start of epoch 4\n",
      "Epoch 4 Step 0 Loss 4.0822Accuracy 0.4211\n",
      "Epoch 4 Step 50 Loss 4.1306Accuracy 0.3878\n",
      "Epoch 4 Step 100 Loss 4.0237Accuracy 0.3974\n",
      "Epoch 4: Training Loss 3.9567, Training Accuracy 0.4062, Validation Loss 3.7577\n",
      "Saved checkpoint at epoch 4\n",
      "\n",
      "Start of epoch 5\n",
      "Epoch 5 Step 0 Loss 3.6365Accuracy 0.4596\n",
      "Epoch 5 Step 50 Loss 3.6998Accuracy 0.4275\n",
      "Epoch 5 Step 100 Loss 3.5979Accuracy 0.4399\n",
      "Epoch 5: Training Loss 3.5390, Training Accuracy 0.4474, Validation Loss 3.5812\n",
      "Saved checkpoint at epoch 5\n",
      "\n",
      "Start of epoch 6\n",
      "Epoch 6 Step 0 Loss 3.2963Accuracy 0.4842\n",
      "Epoch 6 Step 50 Loss 3.3610Accuracy 0.4645\n",
      "Epoch 6 Step 100 Loss 3.2687Accuracy 0.4746\n",
      "Epoch 6: Training Loss 3.2212, Training Accuracy 0.4809, Validation Loss 3.4642\n",
      "Saved checkpoint at epoch 6\n",
      "\n",
      "Start of epoch 7\n",
      "Epoch 7 Step 0 Loss 3.0430Accuracy 0.4807\n",
      "Epoch 7 Step 50 Loss 3.0337Accuracy 0.5009\n",
      "Epoch 7 Step 100 Loss 2.9419Accuracy 0.5093\n",
      "Epoch 7: Training Loss 2.9110, Training Accuracy 0.5129, Validation Loss 3.2690\n",
      "Saved checkpoint at epoch 7\n",
      "\n",
      "Start of epoch 8\n",
      "Epoch 8 Step 0 Loss 2.6468Accuracy 0.5860\n",
      "Epoch 8 Step 50 Loss 2.7118Accuracy 0.5343\n",
      "Epoch 8 Step 100 Loss 2.6402Accuracy 0.5398\n",
      "Epoch 8: Training Loss 2.6171, Training Accuracy 0.5437, Validation Loss 3.1059\n",
      "Saved checkpoint at epoch 8\n",
      "\n",
      "Start of epoch 9\n",
      "Epoch 9 Step 0 Loss 2.3974Accuracy 0.5825\n",
      "Epoch 9 Step 50 Loss 2.4261Accuracy 0.5659\n",
      "Epoch 9 Step 100 Loss 2.3640Accuracy 0.5729\n",
      "Epoch 9: Training Loss 2.3425, Training Accuracy 0.5759, Validation Loss 3.0701\n",
      "Saved checkpoint at epoch 9\n",
      "\n",
      "Start of epoch 10\n",
      "Epoch 10 Step 0 Loss 2.2345Accuracy 0.5965\n",
      "Epoch 10 Step 50 Loss 2.1378Accuracy 0.6009\n",
      "Epoch 10 Step 100 Loss 2.0841Accuracy 0.6092\n",
      "Epoch 10: Training Loss 2.0638, Training Accuracy 0.6114, Validation Loss 3.0135\n",
      "Saved checkpoint at epoch 10\n",
      "\n",
      "Start of epoch 11\n",
      "Epoch 11 Step 0 Loss 1.9114Accuracy 0.6526\n",
      "Epoch 11 Step 50 Loss 1.8420Accuracy 0.6449\n",
      "Epoch 11 Step 100 Loss 1.7775Accuracy 0.6574\n",
      "Epoch 11: Training Loss 1.7661, Training Accuracy 0.6594, Validation Loss 3.0383\n",
      "Saved checkpoint at epoch 11\n",
      "\n",
      "Start of epoch 12\n",
      "Epoch 12 Step 0 Loss 1.5912Accuracy 0.7018\n",
      "Epoch 12 Step 50 Loss 1.6120Accuracy 0.6825\n",
      "Epoch 12 Step 100 Loss 1.5435Accuracy 0.6948\n",
      "Epoch 12: Training Loss 1.5095, Training Accuracy 0.7011, Validation Loss 2.9139\n",
      "Saved checkpoint at epoch 12\n",
      "\n",
      "Start of epoch 13\n",
      "Epoch 13 Step 0 Loss 1.4166Accuracy 0.6842\n",
      "Epoch 13 Step 50 Loss 1.3075Accuracy 0.7434\n",
      "Epoch 13 Step 100 Loss 1.2402Accuracy 0.7563\n",
      "Epoch 13: Training Loss 1.2015, Training Accuracy 0.7652, Validation Loss 2.8551\n",
      "Saved checkpoint at epoch 13\n",
      "\n",
      "Start of epoch 14\n",
      "Epoch 14 Step 0 Loss 1.0508Accuracy 0.7544\n",
      "Epoch 14 Step 50 Loss 1.0424Accuracy 0.7981\n",
      "Epoch 14 Step 100 Loss 0.9709Accuracy 0.8146\n",
      "Epoch 14: Training Loss 0.9405, Training Accuracy 0.8211, Validation Loss 2.8540\n",
      "Saved checkpoint at epoch 14\n",
      "\n",
      "Start of epoch 15\n",
      "Epoch 15 Step 0 Loss 0.7211Accuracy 0.8632\n",
      "Epoch 15 Step 50 Loss 0.8590Accuracy 0.8352\n",
      "Epoch 15 Step 100 Loss 0.7872Accuracy 0.8526\n",
      "Epoch 15: Training Loss 0.7618, Training Accuracy 0.8581, Validation Loss 2.9492\n",
      "Saved checkpoint at epoch 15\n",
      "\n",
      "Start of epoch 16\n",
      "Epoch 16 Step 0 Loss 0.6483Accuracy 0.9018\n",
      "Epoch 16 Step 50 Loss 0.6783Accuracy 0.8732\n",
      "Epoch 16 Step 100 Loss 0.6253Accuracy 0.8840\n",
      "Epoch 16: Training Loss 0.6014, Training Accuracy 0.8896, Validation Loss 2.8945\n",
      "Saved checkpoint at epoch 16\n",
      "\n",
      "Start of epoch 17\n",
      "Epoch 17 Step 0 Loss 0.4766Accuracy 0.9158\n",
      "Epoch 17 Step 50 Loss 0.4890Accuracy 0.9153\n",
      "Epoch 17 Step 100 Loss 0.4625Accuracy 0.9184\n",
      "Epoch 17: Training Loss 0.4492, Training Accuracy 0.9209, Validation Loss 2.8930\n",
      "Saved checkpoint at epoch 17\n",
      "\n",
      "Start of epoch 18\n",
      "Epoch 18 Step 0 Loss 0.3377Accuracy 0.9439\n",
      "Epoch 18 Step 50 Loss 0.3750Accuracy 0.9294\n",
      "Epoch 18 Step 100 Loss 0.3451Accuracy 0.9352\n",
      "Epoch 18: Training Loss 0.3402, Training Accuracy 0.9358, Validation Loss 2.9857\n",
      "Saved checkpoint at epoch 18\n",
      "\n",
      "Start of epoch 19\n",
      "Epoch 19 Step 0 Loss 0.2962Accuracy 0.9228\n",
      "Epoch 19 Step 50 Loss 0.3122Accuracy 0.9345\n",
      "Epoch 19 Step 100 Loss 0.2716Accuracy 0.9446\n",
      "Epoch 19: Training Loss 0.2702, Training Accuracy 0.9446, Validation Loss 3.2212\n",
      "Saved checkpoint at epoch 19\n",
      "\n",
      "Start of epoch 20\n",
      "Epoch 20 Step 0 Loss 0.2886Accuracy 0.9333\n",
      "Epoch 20 Step 50 Loss 0.2695Accuracy 0.9419\n",
      "Epoch 20 Step 100 Loss 0.2447Accuracy 0.9474\n",
      "Epoch 20: Training Loss 0.2396, Training Accuracy 0.9484, Validation Loss 3.1535\n",
      "Saved checkpoint at epoch 20\n",
      "Total time taken: 19.41s\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    train_loss.reset_states()\n",
    "    train_accuracy.reset_states()\n",
    "    val_loss.reset_states()\n",
    "    print(\"\\nStart of epoch %d\" % (epoch + 1))\n",
    "    \n",
    "    start_time = time()\n",
    "    # Iterate over the dataset batches\n",
    "    for step, (train_batchX, train_batchY) in enumerate(train_dataset):\n",
    "        # Define the encoder and decoder inputs, and the decoder output\n",
    "        encoder_input = train_batchX[:, 1:]\n",
    "        decoder_input = train_batchY[:, :-1]\n",
    "        decoder_output = train_batchY[:, 1:]\n",
    "        \n",
    "        train_step(encoder_input, decoder_input, decoder_output)\n",
    "        \n",
    "        if step % 50 == 0:\n",
    "            print(f\"Epoch {epoch+1} Step {step} Loss {train_loss.result():.4f}\" + f\"Accuracy {train_accuracy.result():.4f}\")\n",
    "    \n",
    "    \n",
    "    # Run a validation step after every epoch of training\n",
    "    for val_batchX, val_batchY in val_dataset:\n",
    "    # Define the encoder and decoder inputs, and the decoder output\n",
    "        encoder_input = val_batchX[:, 1:]\n",
    "        decoder_input = val_batchY[:, :-1]\n",
    "        decoder_output = val_batchY[:, 1:]\n",
    "        # Generate a prediction\n",
    "        prediction = training_model(encoder_input, decoder_input, training=False)\n",
    "        # Compute the validation loss\n",
    "        loss = loss_fcn(decoder_output, prediction)\n",
    "        val_loss(loss)\n",
    "            \n",
    "    # Print epoch number and loss value at the end of every epoch\n",
    "    print(f\"Epoch {epoch+1}: Training Loss {train_loss.result():.4f}, \" + f\"Training Accuracy {train_accuracy.result():.4f}, \" + f\"Validation Loss {val_loss.result():.4f}\")\n",
    "    \n",
    "    # Save a checkpoint after every epoch\n",
    "    if (epoch + 1) % 1 == 0:\n",
    "        save_path = ckpt_manager.save()\n",
    "        print(f\"Saved checkpoint at epoch {epoch+1}\")\n",
    "        \n",
    "        # Save the trained model weights\n",
    "        training_model.save_weights(\"weights/wghts\" + str(epoch + 1) + \".ckpt\")\n",
    "        train_loss_dict[epoch] = train_loss.result()\n",
    "        \n",
    "        val_loss_dict[epoch] = val_loss.result()\n",
    "# Save the training loss values\n",
    "with open('./train_loss.pkl', 'wb') as file:\n",
    "    dump(train_loss_dict, file)\n",
    "# Save the validation loss values\n",
    "with open('./val_loss.pkl', 'wb') as file:\n",
    "    dump(val_loss_dict, file)\n",
    "print(\"Total time taken: %.2fs\" % (time() - start_time))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-22T04:08:32.703852Z",
     "start_time": "2025-01-22T04:02:50.307171Z"
    }
   },
   "id": "2140778ff65839d6"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Plotting the training and validation loss curves"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b3d8217878a4481e"
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "outputs": [],
   "source": [
    "from matplotlib.pylab import plt\n",
    "from numpy import arange"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-22T04:08:32.709571Z",
     "start_time": "2025-01-22T04:08:32.704687Z"
    }
   },
   "id": "957c1406271ba5e7"
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the training and validation loss dictionaries\n",
    "train_loss = load(open('train_loss.pkl', 'rb'))\n",
    "val_loss = load(open('val_loss.pkl', 'rb'))\n",
    "\n",
    "# Retrieve each dictionary's values\n",
    "train_values = train_loss.values()\n",
    "val_values = val_loss.values()\n",
    "\n",
    "# Generate a sequence of integers to represent the epoch numbers\n",
    "epochs = range(1, 21)\n",
    "\n",
    "\n",
    "# Plot and label the training and validation loss values\n",
    "plt.plot(epochs, train_values, label='Training Loss')\n",
    "plt.plot(epochs, val_values, label='Validation Loss')\n",
    "\n",
    "\n",
    "# Add in a title and axes labels\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "\n",
    "# Set the tick locations\n",
    "plt.xticks(arange(0, 21, 2))\n",
    "\n",
    "# Display the plot\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-22T04:08:32.826387Z",
     "start_time": "2025-01-22T04:08:32.708973Z"
    }
   },
   "id": "a93e98c314ae4c49"
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "outputs": [
    {
     "data": {
      "text/plain": "{0: <tf.Tensor: shape=(), dtype=float32, numpy=6.6854577>,\n 1: <tf.Tensor: shape=(), dtype=float32, numpy=5.2045183>,\n 2: <tf.Tensor: shape=(), dtype=float32, numpy=4.4632397>,\n 3: <tf.Tensor: shape=(), dtype=float32, numpy=3.9566977>,\n 4: <tf.Tensor: shape=(), dtype=float32, numpy=3.5390325>,\n 5: <tf.Tensor: shape=(), dtype=float32, numpy=3.2211778>,\n 6: <tf.Tensor: shape=(), dtype=float32, numpy=2.911011>,\n 7: <tf.Tensor: shape=(), dtype=float32, numpy=2.6170697>,\n 8: <tf.Tensor: shape=(), dtype=float32, numpy=2.3425>,\n 9: <tf.Tensor: shape=(), dtype=float32, numpy=2.063792>,\n 10: <tf.Tensor: shape=(), dtype=float32, numpy=1.7661085>,\n 11: <tf.Tensor: shape=(), dtype=float32, numpy=1.5094538>,\n 12: <tf.Tensor: shape=(), dtype=float32, numpy=1.2014846>,\n 13: <tf.Tensor: shape=(), dtype=float32, numpy=0.94053984>,\n 14: <tf.Tensor: shape=(), dtype=float32, numpy=0.7618492>,\n 15: <tf.Tensor: shape=(), dtype=float32, numpy=0.6013766>,\n 16: <tf.Tensor: shape=(), dtype=float32, numpy=0.44922185>,\n 17: <tf.Tensor: shape=(), dtype=float32, numpy=0.34024557>,\n 18: <tf.Tensor: shape=(), dtype=float32, numpy=0.2702483>,\n 19: <tf.Tensor: shape=(), dtype=float32, numpy=0.2395866>}"
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loss"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-22T04:08:32.830172Z",
     "start_time": "2025-01-22T04:08:32.826488Z"
    }
   },
   "id": "800dcd00f513c978"
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "outputs": [
    {
     "data": {
      "text/plain": "dict_values([<tf.Tensor: shape=(), dtype=float32, numpy=6.6854577>, <tf.Tensor: shape=(), dtype=float32, numpy=5.2045183>, <tf.Tensor: shape=(), dtype=float32, numpy=4.4632397>, <tf.Tensor: shape=(), dtype=float32, numpy=3.9566977>, <tf.Tensor: shape=(), dtype=float32, numpy=3.5390325>, <tf.Tensor: shape=(), dtype=float32, numpy=3.2211778>, <tf.Tensor: shape=(), dtype=float32, numpy=2.911011>, <tf.Tensor: shape=(), dtype=float32, numpy=2.6170697>, <tf.Tensor: shape=(), dtype=float32, numpy=2.3425>, <tf.Tensor: shape=(), dtype=float32, numpy=2.063792>, <tf.Tensor: shape=(), dtype=float32, numpy=1.7661085>, <tf.Tensor: shape=(), dtype=float32, numpy=1.5094538>, <tf.Tensor: shape=(), dtype=float32, numpy=1.2014846>, <tf.Tensor: shape=(), dtype=float32, numpy=0.94053984>, <tf.Tensor: shape=(), dtype=float32, numpy=0.7618492>, <tf.Tensor: shape=(), dtype=float32, numpy=0.6013766>, <tf.Tensor: shape=(), dtype=float32, numpy=0.44922185>, <tf.Tensor: shape=(), dtype=float32, numpy=0.34024557>, <tf.Tensor: shape=(), dtype=float32, numpy=0.2702483>, <tf.Tensor: shape=(), dtype=float32, numpy=0.2395866>])"
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_values"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-22T04:08:32.834606Z",
     "start_time": "2025-01-22T04:08:32.830608Z"
    }
   },
   "id": "ae31b781685c55e9"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Inferencing the transformer model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b077f3758ff4d7e2"
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "outputs": [],
   "source": [
    "# Define the model parameters\n",
    "h = 8 # Number of self-attention heads\n",
    "d_k = 64 # Dimensionality of the linearly projected queries and keys\n",
    "d_v = 64 # Dimensionality of the linearly projected values\n",
    "d_model = 512 # Dimensionality of model layers' outputs\n",
    "d_ff = 2048 # Dimensionality of the inner fully connected layer\n",
    "n = 6 # Number of layers in the encoder stack\n",
    "\n",
    "# Define the dataset parameters\n",
    "enc_seq_length = 7 # Encoder sequence length\n",
    "dec_seq_length = 10 # Decoder sequence length\n",
    "enc_vocab_size = 2195 # Encoder vocabulary size\n",
    "dec_vocab_size = 3466 # Decoder vocabulary size\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-22T04:57:59.445272Z",
     "start_time": "2025-01-22T04:57:59.435446Z"
    }
   },
   "id": "62f3624355a9ce25"
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "outputs": [],
   "source": [
    "# Create model\n",
    "inferencing_model = TransformerModel(enc_vocab_size, dec_vocab_size, enc_seq_length,dec_seq_length, h, d_k, d_v, d_model, d_ff, n, 0)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-22T04:58:02.969445Z",
     "start_time": "2025-01-22T04:57:59.829964Z"
    }
   },
   "id": "afccf36b790a0dd"
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "outputs": [],
   "source": [
    "class Translate(Module):\n",
    "    def __init__(self, inferencing_model, **kwargs):\n",
    "       super().__init__(**kwargs)\n",
    "       self.transformer = inferencing_model\n",
    "    \n",
    "    def load_tokenizer(self, name):\n",
    "        with open(name, 'rb') as handle:\n",
    "            return load(handle)\n",
    "        \n",
    "    def __call__(self, sentence):\n",
    "        # Append start and end of string tokens to the input sentence\n",
    "        sentence[0] = \"<START> \" + sentence[0] + \" <EOS>\"\n",
    "        \n",
    "        # Load encoder and decoder tokenizers\n",
    "        enc_tokenizer = self.load_tokenizer('enc_tokenizer.pkl')\n",
    "        dec_tokenizer = self.load_tokenizer('dec_tokenizer.pkl')\n",
    "        \n",
    "        # Prepare the input sentence by tokenizing, padding and converting to tensor\n",
    "        encoder_input = enc_tokenizer.texts_to_sequences(sentence)\n",
    "        encoder_input = pad_sequences(encoder_input,maxlen=enc_seq_length, padding='post')\n",
    "        encoder_input = convert_to_tensor(encoder_input, dtype=int64)\n",
    "        \n",
    "        # Prepare the output <START> token by tokenizing, and converting to tensor\n",
    "        output_start = dec_tokenizer.texts_to_sequences([\"<START>\"])\n",
    "        output_start = convert_to_tensor(output_start[0], dtype=int64)\n",
    "        \n",
    "        # Prepare the output <EOS> token by tokenizing, and converting to tensor\n",
    "        output_end = dec_tokenizer.texts_to_sequences([\"<EOS>\"])\n",
    "        output_end = convert_to_tensor(output_end[0], dtype=int64)\n",
    "        \n",
    "        # Prepare the output array of dynamic size\n",
    "        decoder_output = TensorArray(dtype=int64, size=0, dynamic_size=True)\n",
    "        decoder_output = decoder_output.write(0, output_start)\n",
    "        \n",
    "        for i in range(dec_seq_length):\n",
    "            # Predict an output token\n",
    "            prediction = self.transformer(encoder_input,transpose(decoder_output.stack()),training=False)\n",
    "            prediction = prediction[:, -1, :]\n",
    "            \n",
    "            # Select the prediction with the highest score\n",
    "            predicted_id = argmax(prediction, axis=-1)\n",
    "            predicted_id = predicted_id[0][newaxis]\n",
    "            \n",
    "            # Write the selected prediction to the output array at the next\n",
    "            # available index\n",
    "            decoder_output = decoder_output.write(i + 1, predicted_id)\n",
    "            \n",
    "            # Break if an <EOS> token is predicted\n",
    "            if predicted_id == output_end:\n",
    "                break\n",
    "                \n",
    "        output = transpose(decoder_output.stack())[0]\n",
    "        output = output.numpy()\n",
    "        output_str = []\n",
    "        \n",
    "        # Decode the predicted tokens into an output string\n",
    "        for i in range(output.shape[0]):\n",
    "            key = output[i]\n",
    "            output_str.append(dec_tokenizer.index_word[key])\n",
    "        \n",
    "        return output_str\n",
    "        "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-22T04:58:02.975149Z",
     "start_time": "2025-01-22T04:58:02.972964Z"
    }
   },
   "id": "3a81d603ae0df15f"
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "outputs": [],
   "source": [
    "# Sentence to translate\n",
    "sentence = ['cat sat on the mat']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-22T06:57:40.124098Z",
     "start_time": "2025-01-22T06:57:40.119937Z"
    }
   },
   "id": "ee78134a66e769de"
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "outputs": [
    {
     "data": {
      "text/plain": "<tensorflow.python.checkpoint.checkpoint.CheckpointLoadStatus at 0x167971ea0>"
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the trained model's weights at the specified epoch\n",
    "inferencing_model.load_weights('weights/wghts18.ckpt')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-22T06:57:41.888585Z",
     "start_time": "2025-01-22T06:57:41.341582Z"
    }
   },
   "id": "7dc9635bd7d81b2b"
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "outputs": [],
   "source": [
    "# Create a new instance of the 'Translate' class\n",
    "translator = Translate(inferencing_model)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-22T06:57:42.714595Z",
     "start_time": "2025-01-22T06:57:42.707162Z"
    }
   },
   "id": "b5c819ace40f8d9c"
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['start', 'ich', 'bin', 'mir', 'kommen', 'mir', 'ihnen', 'eos']\n"
     ]
    }
   ],
   "source": [
    "print(translator(sentence))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-22T06:57:45.329490Z",
     "start_time": "2025-01-22T06:57:43.348414Z"
    }
   },
   "id": "b5a9896c3f68be4a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "ac5bb415afc92c5b"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
