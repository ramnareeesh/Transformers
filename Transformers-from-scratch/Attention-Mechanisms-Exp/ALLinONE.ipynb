{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMlf6nWeOkM2zegqaESLs/t"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"Sg64jx10XwI3"},"outputs":[],"source":["#SCALED DOT PRODUCT\n","from IPython import get_ipython\n","from IPython.display import display\n","from tensorflow import matmul, cast, float32, math\n","from tensorflow.math import sqrt\n","from tensorflow.keras.layers import Layer\n","from tensorflow.keras.activations import softmax\n","import numpy as np\n","\n","class DotProductAttention(Layer):\n","  def __init__(self, **kwargs):\n","    super().__init__(**kwargs)\n","\n","  def call(self, queries, keys, values, *, d_k, mask=None):\n","    scores = matmul(queries, keys, transpose_b=True) / sqrt(cast(d_k, float32))\n","    if mask is not None:\n","      scores += -1e9 * mask\n","    weights = softmax(scores)\n","    return matmul(weights, values)\n","\n","batch_size = 32\n","input_seq_length = 10\n","d_k = 64\n","d_v = 64\n","\n","random = np.random.default_rng(seed=42)\n","queries = random.random((batch_size, input_seq_length, d_k))\n","keys = random.random((batch_size, input_seq_length, d_k))\n","values = random.random((batch_size, input_seq_length, d_v))\n","attention = DotProductAttention()\n","print(attention(queries, keys, values, d_k=d_k))"]},{"cell_type":"code","source":["from IPython import get_ipython\n","from IPython.display import display\n","# %%\n","from tensorflow import math, matmul, reshape, shape, transpose, cast, float32, concat\n","from tensorflow.keras.layers import Dense, Layer\n","from tensorflow.keras.backend import softmax\n","# Implementing the Scaled-Dot Product Attention\n","\n","class DotProductAttention(Layer):\n","  def __init__(self, **kwargs):\n","    super().__init__(**kwargs)\n","\n","  def call(self, queries, keys, values, mask=None):\n","    d_k = queries.shape[-1]\n","    # Scoring the queries against the keys after transposing the latter, and scaling\n","    scores = matmul(queries, keys, transpose_b=True) / math.sqrt(cast(d_k, float32))\n","    # Apply mask to the attention scores\n","    if mask is not None:\n","      scores += -1e9 * mask\n","    # Computing the weights by a softmax operation\n","    weights = softmax(scores)\n","    # Computing the attention by a weighted sum of the value vectors\n","    return matmul(weights, values)\n","    # Implementing the Multi-Head Attention\n","\n","class MultiHeadAttention(Layer):\n","  def __init__(self, h, d_k, d_v, d_model, **kwargs):\n","    super().__init__(**kwargs)\n","    self.attention = DotProductAttention() # Scaled dot product attention\n","    self.heads = h # Number of attention heads to use\n","    self.d_k = d_k # Dimensionality of the linearly projected queries and keys\n","    self.d_v = d_v # Dimensionality of the linearly projected values\n","    self.d_model = d_model # Dimensionality of the model\n","    self.W_q = Dense(d_k) # Learned projection matrix for the queries\n","    self.W_k = Dense(d_k) # Learned projection matrix for the keys\n","    self.W_v = Dense(d_v) # Learned projection matrix for the values\n","    self.W_o = Dense(d_model) # Learned projection matrix for the multi-head output\n","\n","  def reshape_tensor(self, x, heads, flag):\n","    if flag:\n","      # Tensor shape after reshaping and transposing:\n","      # (batch_size, heads, seq_length, -1)\n","      x = reshape(x, shape=(shape(x)[0], shape(x)[1], heads, -1))\n","      x = transpose(x, perm=(0, 2, 1, 3))\n","    else:\n","        x = transpose(x, perm=(0, 2, 1, 3))\n","        x_shape = shape(x)\n","        new_shape = (x_shape[0], x_shape[1], x_shape[2] * x_shape[3])\n","        x = reshape(x, new_shape)\n","\n","    return x\n","\n","  def call(self, queries, keys, values, mask=None):\n","    # Rearrange the queries to be able to compute all heads in parallel\n","    q_reshaped = self.reshape_tensor(self.W_q(queries), self.heads, True)\n","    # Resulting tensor shape: (batch_size, heads, input_seq_length, -1)\n","    # Rearrange the keys to be able to compute all heads in parallel\n","    k_reshaped = self.reshape_tensor(self.W_k(keys), self.heads, True)\n","    # Resulting tensor shape: (batch_size, heads, input_seq_length, -1)\n","    # Rearrange the values to be able to compute all heads in parallel\n","    v_reshaped = self.reshape_tensor(self.W_v(values), self.heads, True)\n","    # Resulting tensor shape: (batch_size, heads, input_seq_length, -1)\n","    # Compute the multi-head attention output using the reshaped queries,\n","    # keys, and values\n","    o_reshaped = self.attention(q_reshaped, k_reshaped, v_reshaped, mask=mask)\n","    # Resulting tensor shape: (batch_size, heads, input_seq_length, -1)\n","    # Rearrange back the output into concatenated form\n","    output = self.reshape_tensor(o_reshaped, self.heads, False)\n","    # Resulting tensor shape: (batch_size, input_seq_length, d_model)\n","    return self.W_o(output)\n","# %%\n","from numpy import random\n","input_seq_length = 5 # Maximum length of the input sequence\n","h = 8 # Number of self-attention heads\n","d_k = 64 # Dimensionality of the linearly projected queries and keys\n","d_v = 64 # Dimensionality of the linearly projected values\n","d_model = 512 # Dimensionality of the model sub-layers' outputs\n","batch_size = 64 # Batch size from the training process\n","queries = random.random((batch_size, input_seq_length, d_k))\n","keys = random.random((batch_size, input_seq_length, d_k))\n","values = random.random((batch_size, input_seq_length, d_v))\n","multihead_attention = MultiHeadAttention(h, d_k, d_v, d_model)\n","print(multihead_attention(queries, keys, values))"],"metadata":{"id":"mGph6WV6X-mP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import tensorflow as tf\n","\n","class BahdanauAttention(tf.keras.layers.Layer):\n","    def __init__(self, units):\n","        super(BahdanauAttention, self).__init__()\n","        self.W1 = tf.keras.layers.Dense(units)  # Dense layer for the encoder hidden states\n","        self.W2 = tf.keras.layers.Dense(units)  # Dense layer for the decoder hidden state\n","        self.V = tf.keras.layers.Dense(1)       # Dense layer to compute alignment scores\n","\n","    def call(self, query, values):\n","        \"\"\"\n","        Args:\n","            query: Decoder hidden state (shape: [batch_size, hidden_size]).\n","            values: Encoder outputs (shape: [batch_size, seq_len, hidden_size]).\n","        Returns:\n","            context_vector: Weighted sum of encoder outputs (shape: [batch_size, hidden_size]).\n","            attention_weights: Attention weights (shape: [batch_size, seq_len]).\n","        \"\"\"\n","        # Add time axis to query for broadcasting (shape: [batch_size, 1, hidden_size])\n","        query_with_time_axis = tf.expand_dims(query, 1)\n","\n","        # Compute the alignment scores (shape: [batch_size, seq_len, 1])\n","        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(query_with_time_axis)))\n","\n","        # Remove the last axis (shape: [batch_size, seq_len])\n","        attention_weights = tf.nn.softmax(score, axis=1)\n","\n","        # Compute the context vector as the weighted sum of values (shape: [batch_size, hidden_size])\n","        context_vector = tf.reduce_sum(attention_weights * values, axis=1)\n","\n","        return context_vector, attention_weights\n","\n","# Example usage\n","if __name__ == \"__main__\":\n","    # Define batch size, sequence length, and hidden size\n","    batch_size = 64\n","    seq_len = 10\n","    hidden_size = 256\n","    attention_units = 128\n","\n","    # Instantiate the attention layer\n","    attention = BahdanauAttention(units=attention_units)\n","\n","    # Simulated encoder outputs (values) and decoder hidden state (query)\n","    encoder_outputs = tf.random.normal([batch_size, seq_len, hidden_size])\n","    decoder_hidden_state = tf.random.normal([batch_size, hidden_size])\n","\n","    # Apply the attention mechanism\n","    context_vector, attention_weights = attention(decoder_hidden_state, encoder_outputs)\n","\n","    print(\"Context vector shape:\", context_vector.shape)  # Expected: [batch_size, hidden_size]\n","    print(\"Attention weights shape:\", attention_weights.shape)  # Expected: [batch_size, seq_len]\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4PxLmp2TYCnj","executionInfo":{"status":"ok","timestamp":1737777741452,"user_tz":-330,"elapsed":13628,"user":{"displayName":"ARAVINDH","userId":"17493391128659897074"}},"outputId":"21eaccd5-ecbf-4b5f-e784-007192135932"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Context vector shape: (64, 256)\n","Attention weights shape: (64, 10, 1)\n"]}]},{"cell_type":"code","source":["decoder_hidden_state"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"s5vM0tNxoBc7","executionInfo":{"status":"ok","timestamp":1737777763796,"user_tz":-330,"elapsed":502,"user":{"displayName":"ARAVINDH","userId":"17493391128659897074"}},"outputId":"eb88e94e-4659-4104-ae34-d3fe8e4883f6"},"execution_count":2,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(64, 256), dtype=float32, numpy=\n","array([[-1.0162307 ,  0.2542984 , -1.1475248 , ...,  1.2792453 ,\n","         0.08146834,  1.110824  ],\n","       [ 0.09520556,  0.15147667,  0.30544576, ..., -0.96792173,\n","         0.29477042, -0.31000718],\n","       [ 0.1262379 ,  1.0118979 , -0.391723  , ..., -1.1618767 ,\n","        -0.38834393, -0.6393011 ],\n","       ...,\n","       [ 0.71681315,  0.47909588, -1.1250935 , ..., -2.7017155 ,\n","        -2.9282985 ,  0.15480897],\n","       [ 0.42129475, -0.25539428,  0.5809801 , ...,  2.3172004 ,\n","        -2.638371  , -1.4490504 ],\n","       [ 0.76044065,  0.5144072 , -0.9479262 , ...,  1.6367507 ,\n","         1.2844629 , -0.68204767]], dtype=float32)>"]},"metadata":{},"execution_count":2}]},{"cell_type":"code","source":["decoder_hidden_state.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iZbU5oukoItu","executionInfo":{"status":"ok","timestamp":1737777788334,"user_tz":-330,"elapsed":503,"user":{"displayName":"ARAVINDH","userId":"17493391128659897074"}},"outputId":"ca8fdcd4-d34a-4ffb-e1de-5832defce1c4"},"execution_count":3,"outputs":[{"output_type":"execute_result","data":{"text/plain":["TensorShape([64, 256])"]},"metadata":{},"execution_count":3}]},{"cell_type":"code","source":["import tensorflow as tf\n","from tensorflow.keras.layers import Layer, Dense\n","\n","class LuongAttention(Layer):\n","    def __init__(self, attention_type, hidden_size):\n","        super(LuongAttention, self).__init__()\n","        self.attention_type = attention_type\n","        self.hidden_size = hidden_size\n","\n","        if attention_type == \"general\":\n","            self.attention_weight = Dense(hidden_size)\n","        elif attention_type == \"concat\":\n","            self.attention_weight = Dense(hidden_size)\n","            self.v = tf.Variable(tf.random.normal([hidden_size]), trainable=True)\n","\n","    def score(self, hidden, encoder_outputs):\n","        if self.attention_type == \"dot\":\n","            # Dot product between hidden state and encoder outputs\n","            return tf.matmul(encoder_outputs, tf.expand_dims(hidden, axis=-1))[:, :, 0]\n","\n","        elif self.attention_type == \"general\":\n","            # Linear transformation followed by dot product\n","            energy = self.attention_weight(encoder_outputs)\n","            return tf.matmul(energy, tf.expand_dims(hidden, axis=-1))[:, :, 0]\n","\n","        elif self.attention_type == \"concat\":\n","            # Concatenate hidden state with encoder outputs\n","            hidden_expanded = tf.expand_dims(hidden, axis=1)\n","            hidden_expanded = tf.tile(hidden_expanded, [1, tf.shape(encoder_outputs)[1], 1])\n","            concat_input = tf.concat([hidden_expanded, encoder_outputs], axis=-1)\n","            energy = tf.tanh(self.attention_weight(concat_input))\n","            return tf.reduce_sum(energy * self.v, axis=2)\n","\n","        else:\n","            raise ValueError(\"Unknown attention type: {}\".format(self.attention_type))\n","\n","    def call(self, hidden, encoder_outputs):\n","        # Compute alignment scores\n","        alignment_scores = self.score(hidden, encoder_outputs)\n","\n","        # Softmax normalization to obtain attention weights\n","        attention_weights = tf.nn.softmax(alignment_scores, axis=1)\n","\n","        # Compute the context vector as the weighted sum of encoder outputs\n","        context_vector = tf.matmul(tf.expand_dims(attention_weights, axis=1), encoder_outputs)\n","        context_vector = tf.squeeze(context_vector, axis=1)\n","\n","        return context_vector, attention_weights\n","\n","\n","# Example usage\n","if __name__ == \"__main__\":\n","    batch_size = 2\n","    seq_len = 5\n","    hidden_size = 10\n","\n","    # Simulated inputs\n","    hidden = tf.random.normal([batch_size, hidden_size])  # Decoder hidden state\n","    encoder_outputs = tf.random.normal([batch_size, seq_len, hidden_size])  # Encoder outputs\n","\n","    # Instantiate Luong Attention (dot, general, or concat)\n","    attention_type = \"dot\"  # Options: \"dot\", \"general\", \"concat\"\n","    attention_layer = LuongAttention(attention_type, hidden_size)\n","\n","    # Forward pass\n","    context_vector, attention_weights = attention_layer(hidden, encoder_outputs)\n","\n","    print(\"Context vector:\", context_vector.numpy())\n","    print(\"Attention weights:\", attention_weights.numpy())\n"],"metadata":{"id":"4JGYDm2yYEX9"},"execution_count":null,"outputs":[]}]}