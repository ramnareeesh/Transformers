{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMqUtTur29hnGXFP17xUfTw"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9XJgGCvTS0ki","executionInfo":{"status":"ok","timestamp":1737738672261,"user_tz":-330,"elapsed":14582,"user":{"displayName":"ARAVINDH","userId":"17493391128659897074"}},"outputId":"85726c30-9bfc-4181-c5df-ed0aa34426e4"},"outputs":[{"output_type":"stream","name":"stdout","text":["Context vector: [[ 0.81676483  0.06053445  0.19244039 -0.72452813 -0.7267856   0.856664\n","   1.1044954   1.2002246   0.31152475  0.3382845 ]\n"," [ 2.384232   -1.3005022  -0.0826828  -0.21933986  1.2408372   0.11178041\n","  -0.3798153   0.28194258  0.16247584 -0.6566643 ]]\n","Attention weights: [[1.16724755e-06 1.03445565e-02 5.45606136e-01 4.33148965e-02\n","  4.00733232e-01]\n"," [7.02524900e-01 8.75239402e-06 2.93246597e-01 4.05862648e-03\n","  1.61005635e-04]]\n"]}],"source":["import tensorflow as tf\n","from tensorflow.keras.layers import Layer, Dense\n","\n","class LuongAttention(Layer):\n","    def __init__(self, attention_type, hidden_size):\n","        super(LuongAttention, self).__init__()\n","        self.attention_type = attention_type\n","        self.hidden_size = hidden_size\n","\n","        if attention_type == \"general\":\n","            self.attention_weight = Dense(hidden_size)\n","        elif attention_type == \"concat\":\n","            self.attention_weight = Dense(hidden_size)\n","            self.v = tf.Variable(tf.random.normal([hidden_size]), trainable=True)\n","\n","    def score(self, hidden, encoder_outputs):\n","        if self.attention_type == \"dot\":\n","            # Dot product between hidden state and encoder outputs\n","            return tf.matmul(encoder_outputs, tf.expand_dims(hidden, axis=-1))[:, :, 0]\n","\n","        elif self.attention_type == \"general\":\n","            # Linear transformation followed by dot product\n","            energy = self.attention_weight(encoder_outputs)\n","            return tf.matmul(energy, tf.expand_dims(hidden, axis=-1))[:, :, 0]\n","\n","        elif self.attention_type == \"concat\":\n","            # Concatenate hidden state with encoder outputs\n","            hidden_expanded = tf.expand_dims(hidden, axis=1)\n","            hidden_expanded = tf.tile(hidden_expanded, [1, tf.shape(encoder_outputs)[1], 1])\n","            concat_input = tf.concat([hidden_expanded, encoder_outputs], axis=-1)\n","            energy = tf.tanh(self.attention_weight(concat_input))\n","            return tf.reduce_sum(energy * self.v, axis=2)\n","\n","        else:\n","            raise ValueError(\"Unknown attention type: {}\".format(self.attention_type))\n","\n","    def call(self, hidden, encoder_outputs):\n","        # Compute alignment scores\n","        alignment_scores = self.score(hidden, encoder_outputs)\n","\n","        # Softmax normalization to obtain attention weights\n","        attention_weights = tf.nn.softmax(alignment_scores, axis=1)\n","\n","        # Compute the context vector as the weighted sum of encoder outputs\n","        context_vector = tf.matmul(tf.expand_dims(attention_weights, axis=1), encoder_outputs)\n","        context_vector = tf.squeeze(context_vector, axis=1)\n","\n","        return context_vector, attention_weights\n","\n","\n","# Example usage\n","if __name__ == \"__main__\":\n","    batch_size = 2\n","    seq_len = 5\n","    hidden_size = 10\n","\n","    # Simulated inputs\n","    hidden = tf.random.normal([batch_size, hidden_size])  # Decoder hidden state\n","    encoder_outputs = tf.random.normal([batch_size, seq_len, hidden_size])  # Encoder outputs\n","\n","    # Instantiate Luong Attention (dot, general, or concat)\n","    attention_type = \"dot\"  # Options: \"dot\", \"general\", \"concat\"\n","    attention_layer = LuongAttention(attention_type, hidden_size)\n","\n","    # Forward pass\n","    context_vector, attention_weights = attention_layer(hidden, encoder_outputs)\n","\n","    print(\"Context vector:\", context_vector.numpy())\n","    print(\"Attention weights:\", attention_weights.numpy())\n"]}]}